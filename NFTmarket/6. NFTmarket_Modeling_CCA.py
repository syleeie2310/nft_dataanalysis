# Databricks notebook source
import numpy as np
import pandas as pd

# COMMAND ----------

# MAGIC %md
# MAGIC # ë°ì´í„° ë¡œë“œ

# COMMAND ----------

data = pd.read_csv('/dbfs/FileStore/nft/nft_market_cleaned/total_220222_cleaned.csv', index_col = "Date", parse_dates=True, thousands=',')

# COMMAND ----------

data.info()

# COMMAND ----------

data.head()

# COMMAND ----------

data.tail()

# COMMAND ----------



# COMMAND ----------

from statsmodels.tsa.ar_model import AR
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf, acf, pacf
import matplotlib.pyplot as plt
import seaborn as sb
from warnings import filterwarnings
filterwarnings("ignore")
plt.style.use("ggplot")
pd.options.display.float_format = '{:.2f}'.format

# COMMAND ----------

# MAGIC %md
# MAGIC # Cross Correlation(ìƒí˜¸ìƒê´€ë¶„ì„)
# MAGIC - ìœ„í‚¤í”¼ë””ì•„: https://en.wikipedia.org/wiki/Cross-correlation
# MAGIC - 1d ë°°ì—´ : statsmodelCCF, numpy.correlate, matplotlib.pyplot.xcorr(numpy.correlate ê¸°ë°˜)
# MAGIC   - https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.ccf.html
# MAGIC   - https://numpy.org/doc/stable/reference/generated/numpy.correlate.html#numpy.correlate
# MAGIC   - numpy.correlateFFTë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¨ë³¼ë£¨ì…˜ì„ ê³„ì‚°í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— í° ë°°ì—´(ì¦‰, n = 1e5)ì—ì„œ ëŠë¦¬ê²Œ ìˆ˜í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° scipy.signal.correlateë°”ëŒì§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# MAGIC - 2d ë°°ì—´ : scipy.signal.correlate2d , scipy.stsci.convolve.correlate2d 
# MAGIC - êµì°¨ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜ëŠ” tê¸°ì˜ íŠ¹ì •(ê¸°ì¤€)ë³€ìˆ˜ xì˜ ê°’(ğ’™ğ’•)ê³¼ t+kê¸°ì— ê´€ì°°ëœ yê°’(ğ’šğ’•+ğ’Œ) ê°„ì˜ ìƒê´€ê´€ê³„ì˜ ì •ë„ë¥¼ ë‚˜íƒ€ëƒ„
# MAGIC - k=0ì¸ ê²½ìš° ì¦‰, ğœ¸ğŸì¸ ê²½ìš°ë¥¼ êµì°¨ìƒê´€ê³„ìˆ˜(cross correlation coefficient)ë¼ê³  í•˜ê³ , kâ‰ 0ì¸ ê²½ìš° ë¥¼ ì‹œì°¨ìƒê´€ê³„ìˆ˜(leads and lags correlationë¼ê³ ë„ í•¨
# MAGIC - êµì°¨ìƒê´€ê³„ìˆ˜ í•´ì„
# MAGIC   - ğœ¸ğŸ> 0 : ë‘ ë³€ìˆ˜ë“¤ì´ ì„œë¡œ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ë³€í™”(pro-cyclical:ê²½ê¸°ìˆœì‘)
# MAGIC   - ğœ¸ğŸ< 0 : ë‘ ë³€ìˆ˜ë“¤ì´ ì„œë¡œ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ë³€í™”(counter-cyclical:ê²½ê¸°ì—­í–‰)
# MAGIC   - ğœ¸ğŸ = 0 : ë‘ ë³€ìˆ˜ë“¤ì´ ì„œë¡œ ê²½ê¸°ì¤‘ë¦½ì 
# MAGIC - ì‹œì°¨ìƒê´€ê³„ìˆ˜ í•´ì„
# MAGIC   - ğœ¸ğ’Œì˜ ê°’ì´ ìµœëŒ€ê°€ ë˜ëŠ” ì‹œì°¨ kê°€ ì–‘(+)ì´ë©´ í•´ë‹¹ë³€ìˆ˜ ğ’šğ’•ëŠ” ğ’™ğ’•ì˜ í›„í–‰ì§€í‘œ
# MAGIC   - ğœ¸ğ’Œì˜ ê°’ì´ ìµœëŒ€ê°€ ë˜ëŠ” ì‹œì°¨ kê°€ ìŒ(-)ì´ë©´ í•´ë‹¹ë³€ìˆ˜ ğ’šğ’•ëŠ” ğ’™ğ’•ì˜ ì„ í–‰ì§€í‘œ
# MAGIC   - ğœ¸ğ’Œì˜ ê°’ì´ ìµœëŒ€ê°€ ë˜ëŠ” ì‹œì°¨ kê°€ 0ì´ë©´ í•´ë‹¹ë³€ìˆ˜ ğ’šğ’•ëŠ” ğ’™ğ’•ì™€ ë™í–‰ì§€í‘œ

# COMMAND ----------

# MAGIC %md
# MAGIC ## 0. CC ë¼ì´ë¸ŒëŸ¬ë¦¬ ìŠ¤í„°ë””

# COMMAND ----------

# MAGIC %md
# MAGIC ### ì˜ˆì œ1 : statsmodel CCF
# MAGIC - adjusted (=unbiased): ì°¸ì´ë©´ êµì°¨ ìƒê´€ì˜ ë¶„ëª¨ëŠ” nkì´ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ nì…ë‹ˆë‹¤.
# MAGIC   - í¸í–¥ë˜ì§€ ì•Šì€ ê²ƒì´ ì°¸ì´ë©´ ìê¸°ê³µë¶„ì‚°ì˜ ë¶„ëª¨ê°€ ì¡°ì •ë˜ì§€ë§Œ ìê¸°ìƒê´€ì€ í¸í–¥ë˜ì§€ ì•Šì€ ì¶”ì •ëŸ‰ì´ ì•„ë‹™ë‹ˆë‹¤.
# MAGIC - fft : Trueì´ë©´ FFT ì»¨ë³¼ë£¨ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ê¸´ ì‹œê³„ì—´ì— ëŒ€í•´ ì„ í˜¸ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

# COMMAND ----------

#define data 
marketing = np.array([3, 4, 5, 5, 7, 9, 13, 15, 12, 10, 8, 8])
revenue = np.array([21, 19, 22, 24, 25, 29, 30, 34, 37, 40, 35, 30]) 

# COMMAND ----------

import statsmodels.api as sm

#calculate cross correlation
sm.tsa.stattools.ccf(marketing, revenue, adjusted=False)

# COMMAND ----------

#  ì‹œì°¨0ì—ì„œ êµì°¨ìƒê´€ì€ 0.771, ì‹œì°¨1ì—ì„œ êµì°¨ìƒê´€ì€ 0.462, ì‹œì°¨2ì—ì„œ êµì°¨ìƒê´€ì€ 0.194, ì‹œì°¨3ì—ì„œ êµì°¨ìƒê´€ì€ -0.061
#  íŠ¹ì • ì›”ì—ì„œ ë§ˆì¼€íŒ…ë¹„ìš©ì„ ì§€ì¶œí•˜ë©´ ë‹¤ìŒ 2ê°œì›”ë™ì•ˆì˜ ìˆ˜ìµì¦ê°€ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.

# COMMAND ----------

# MAGIC %md
# MAGIC ### ì˜ˆì œ2 : numpy.correlate

# COMMAND ----------

import numpy

myArray = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
myArray = numpy.array(myArray)
result = numpy.correlate(myArray, myArray, mode = 'full')
print(result)
print(result.size)
result = result[result.size // 2 :] # ì™„ì „íˆ ê²¹ì¹˜ëŠ” ì§€ì ì¸ ì¤‘ê°„ ì´í›„ë¶€í„° ë´ì•¼í•¨
print(result)

# COMMAND ----------

# MAGIC %md
# MAGIC ### ccfì™€ correlate ì™€ì˜ ì°¨ì´
# MAGIC - https://stackoverflow.com/questions/24616671/numpy-and-statsmodels-give-different-values-when-calculating-correlations-how-t
# MAGIC - ccfëŠ” np.correlate ë² ì´ìŠ¤ì´ì§€ë§Œ, í†µê³„ì ì˜ë¯¸ì—ì„œ ìƒê´€ê´€ê³„ë¥¼ ìœ„í•œ ì¶”ê°€ ì‘ì—…ì„ ìˆ˜í–‰í•¨
# MAGIC - numpyê°€ í‘œì¤€í¸ì°¨ì˜ ê³±ìœ¼ë¡œ ê³µë¶„ì‚°ì„ ì •ê·œí™” í•˜ì§€ ì•ŠìŒ, ê°’ì´ ë„ˆë¬´ í¼
# MAGIC - ccfëŠ” í•©ì„±ê³±ì „ì— ì‹ í˜¸ì˜ í‰ê· ì„ ë¹¼ê³  ê²°ê³¼ë¥¼ ì²«ë²ˆì§¸ ì‹ í˜¸ì˜ ê¸¸ì´ë¡œ ë‚˜ëˆ„ì–´ í†µê³„ì—ì„œì™€ ê°™ì€ ìƒê´€ê´€ê³„ ì •ì˜ì— ë„ë‹¬í•¨
# MAGIC - í†µê³„ ë° ì‹œê³„ì—´ ë¶„ì„ì—ì„œëŠ” êµì°¨ìƒê´€í•¨ìˆ˜ë¥¼ ì •ê·œí™”í•˜ì—¬ ì‹œê°„ì¢…ì† ìƒê´€ê³„ìˆ˜ë¥¼ ì–»ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤.
# MAGIC - ìê¸° ìƒê´€ì„ ìƒê´€ ê´€ê³„ë¡œ í•´ì„í•˜ë©´ í†µê³„ì  ì˜ì¡´ë„ ì˜ ì²™ë„ê°€ ì—†ëŠ” ì¸¡ì •ê°’ì´ ì œê³µ ë˜ê³  ì •ê·œí™”ëŠ” ì¶”ì •ëœ ìê¸° ìƒê´€ì˜ í†µê³„ì  ì†ì„±ì— ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ì •ê·œí™”ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.
# MAGIC - <ì•„ë˜ ì†ŒìŠ¤ì½”ë“œ ì°¸ê³ >
# MAGIC 
# MAGIC ```
# MAGIC def ccovf(x, y, unbiased=True, demean=True):
# MAGIC     n = len(x)
# MAGIC     if demean:
# MAGIC         xo = x - x.mean()
# MAGIC         yo = y - y.mean()
# MAGIC     else:
# MAGIC         xo = x
# MAGIC         yo = y
# MAGIC     if unbiased:
# MAGIC         xi = np.ones(n)
# MAGIC         d = np.correlate(xi, xi, 'full')
# MAGIC     else:
# MAGIC         d = n
# MAGIC     return (np.correlate(xo, yo, 'full') / d)[n - 1:]
# MAGIC 
# MAGIC def ccf(x, y, unbiased=True):
# MAGIC     cvf = ccovf(x, y, unbiased=unbiased, demean=True)
# MAGIC     return cvf / (np.std(x) * np.std(y))
# MAGIC ```

# COMMAND ----------

avgusd = data[feature_classifier(data, 'average_usd')]
avgusd.head()

# COMMAND ----------

avgusd_game = avgusd['game_average_usd']
avgusd_game.head()

# COMMAND ----------

# raw
plt.plot(avgusd_game)

# COMMAND ----------

result = np.correlate(avgusd_game, avgusd_game, 'full')
print(result[result.size // 2 :])

# COMMAND ----------

# numpy.correlate
import numpy as np
from matplotlib import pyplot as plt
from statsmodels.tsa.stattools import ccf

#Calculate correlation using numpy.correlate
def corr(x,y):
    result = numpy.correlate(x, y, mode='full')
    return result[result.size//2:]

#Using numpy i get this
plt.plot(corr(avgusd_game,avgusd_game))

# COMMAND ----------

# statsmodel.ccf
plt.plot(ccf(avgusd_game, avgusd_game, adjusted=False))

# COMMAND ----------

# MAGIC %md
# MAGIC ### êµì°¨ìƒê´€ê³„ìˆ˜ ì‹œê°í™”
# MAGIC - https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xcorr.html
# MAGIC   - ì´ê±´ ì–´ë–»ê²Œ ì»¤ìŠ¤í…€ì„ ëª»í•˜ê² ë‹¤..

# COMMAND ----------

# numply.correlate
result = numpy.correlate(avgusd_game, avgusd_game, mode='full')
npcorr = result[result.size//2:]

nlags = len(npcorr)
leng = len(avgusd_game)

# /* Compute the Significance level */
conf_level = 2 / np.sqrt(nlags)
print('conf_level= ', conf_level)

# /* Draw Plot */
plt.figure(figsize=(30,10), dpi=80)
plt.hlines(0, xmin=0, xmax=leng, color='gray')  # 0 axis
plt.hlines(conf_level, xmin=0, xmax=leng, color='gray')
plt.hlines(-conf_level, xmin=0, xmax=leng, color='gray')

plt.bar(x=np.arange(len(npcorr)), height=npcorr, width=.3)
# plt.bar(x=avgusd_game.index, height=ccs, width=.3) # x ê¸¸ì´ëŠ” ê°™ì€ë°..  ì•ˆë¨..ccsê°’ê³¼ ì¸ë±ìŠ¤ì™€ ë§¤í•‘ì´ ì•ˆë˜ëŠ” ë“¯

# /* Decoration */
plt.title('Cross Correlation Plot <numpy.correlate>', fontsize=22)
plt.xlim(0,len(npcorr))
plt.show()

# COMMAND ----------

# ccf
ccs = ccf(avgusd_game, avgusd_game, adjusted=False)
nlags = len(ccs)
leng = len(avgusd_game)

# /* Compute the Significance level */
conf_level = 2 / np.sqrt(nlags)
print('conf_level= ', conf_level)

# /* Draw Plot */
plt.figure(figsize=(30,10), dpi=80)

plt.hlines(0, xmin=0, xmax=leng, color='gray')  # 0 axis
plt.hlines(conf_level, xmin=0, xmax=leng, color='gray')
plt.hlines(-conf_level, xmin=0, xmax=leng, color='gray')

plt.bar(x=np.arange(len(ccs)), height=ccs, width=.3)
# plt.bar(x=avgusd_game.index, height=ccs, width=.3) # ì•ˆë˜ë„¤..

# /* Decoration */
plt.title('Cross Correlation Plot <statsmodels.CCF>', fontsize=22)
plt.xlim(0,len(ccs))
plt.show()

# COMMAND ----------

# ì•½ 250ì¼ê¹Œì§€ ë‘ë³€ìˆ˜ ë“¤ì´ ì„œë¡œ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ë³€í™”(pro-cyclical:ê²½ê¸°ìˆœì‘)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. CCF-CC êµì°¨ ìƒê´€ê³„ìˆ˜(Cross Correlation)
# MAGIC - avgusd ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµ, ì‹œê°€ì´ì•¡ê³¼ ë¹„êµ
# MAGIC - ë³€ìˆ˜ê°„ ë™í–‰ì„±(comovement) ì¸¡ì •
# MAGIC - ê²½ê¸°ìˆœì‘ì (pro-cyclical) / ê²½ê¸°ì¤‘ë¦½ì (a-cyclical) / ê²½ê¸°ì—­í–‰ì (counter-cyclical)

# COMMAND ----------

# MAGIC %md
# MAGIC ### [í•¨ìˆ˜] êµì°¨ìƒê´€ê³„ìˆ˜ ì°¨íŠ¸ ìƒì„±ê¸°

# COMMAND ----------

# ì¹´í…Œê³ ë¦¬ë³„ í”¼ì²˜ ë¶„ë¥˜ê¸°
def feature_classifier(data, feature):
    col_list = []
    for i in range(len(data.columns)):
        split_col = data.columns[i].split('_', maxsplit=1)[1]
        if split_col == feature:       
            col_list.append(data.columns[i])
        elif split_col == 'all_sales_usd' and feature == 'sales_usd' : #ì½œë ‰í„°ë¸”ë§Œ sales_usdì•ì— allì´ë¶™ì–´ì„œ ë”°ë¡œ ì²˜ë¦¬í•´ì¤Œ
            col_list.append('collectible_all_sales_usd')
        else :
            pass
    return col_list

# COMMAND ----------

# ccf ê³„ìˆ˜ ìƒì„±ê¸°
def ccf_data(data):
    ccfdata = ccf(data, data, adjusted=False)
    return ccfdata

# COMMAND ----------

## ccf ì°¨íŠ¸ ìƒì„±ê¸°
def ccfcc_plot(data, feature):
    # ì¹¼ëŸ¼ ë¦¬ìŠ¤íŠ¸í•¨ìˆ˜ í˜¸ì¶œ
    col_list = feature_classifier(data, feature)
    
    # ccf ê³„ìˆ˜ í•¨ìˆ˜ í˜¸ì¶œ
    for col in col_list:
        ccfdata = ccf_data(data[col])
    
        # /* Compute the Significance level */
        nlags = len(ccfdata)
        conf_level = 2 / np.sqrt(nlags)
#         print('conf_level= ', conf_level)
        print('êµì°¨ìƒê´€ê³„ìˆ˜ê°€ 0ì— ê°€ê¹Œìš´ ì§€ì  = ', min(np.where(ccfdata < 0)[0])-1)
        
        # /* Draw Plot */
        plt.figure(figsize=(30,10), dpi=80)

        plt.hlines(0, xmin=0, xmax=nlags, color='gray')  # 0 axis
        plt.hlines(conf_level, xmin=0, xmax=nlags, color='gray')
        plt.hlines(-conf_level, xmin=0, xmax=nlags, color='gray')

        plt.bar(x=np.arange(nlags), height=ccfdata, width=.3)
        # plt.bar(x=avgusd_game.index, height=ccs, width=.3) # ì•ˆë˜ë„¤..

        # /* Decoration */
        plt.title(f'Cross Correlation Plot <{col}>', fontsize=22)
        plt.xlim(0,nlags)
        plt.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ### ìê¸°êµì°¨ìƒê´€
# MAGIC - ì „ì²´ì¹´í…Œê³ ë¦¬ë³„ ì¸ë±ìŠ¤204~366 (ì•½6ê°œì›”ì—ì„œ 1ë…„ì£¼ê¸°)ê¹Œì§€ ë™í–‰ì„±ì´ ìˆìŒ
# MAGIC - acfì™€ ë™ì¼í•¨

# COMMAND ----------

 # ì „ì²´ ì¹´í…Œê³ ë¦¬- ìê¸°êµì°¨ìƒê´€ ì‹œê°í™”
ccfcc_plot(data, 'average_usd')

# COMMAND ----------

# acfì™€ ë™ì¼í•œë“¯, ë¹„êµí•´ë³´ì. pacfëŠ” 50%ì´í•˜ ê¸¸ì´ë¡œë§Œ ê°€ëŠ¥
# ì ˆë°˜ë§Œ ë´ë„ acfëŠ” ë¹„ìŠ·í•˜ë„¤.

import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

def autoCorrelationF1(data, feature):
    
        # í”¼ì²˜ ë¶„ë¥˜ê¸° í˜¸ì¶œ
    col_list = feature_classifier(data, feature)
    
    for col in col_list:
        series = data[col]

        acf_array = acf(series.dropna(), alpha=0.05, nlags=850) 
        pacf_array = pacf(series.dropna(), alpha=0.05, nlags=850) # 50% ì´í•˜ ê¸¸ì´ê¹Œì§€ë§Œ ê°€ëŠ¥
        array_list = [acf_array, pacf_array]

        fig = make_subplots(rows=1, cols=2)

        for i in range(len(array_list)) :
            corr_array = array_list[i]
            lower_y = corr_array[1][:,0] - corr_array[0]
            upper_y = corr_array[1][:,1] - corr_array[0]
        
            [fig.add_scatter(x=(x,x), y=(0,corr_array[0][x]), mode='lines',line_color='#3f3f3f', row=1, col=i+1)
             for x in range(len(corr_array[0]))]

            fig.add_scatter(x=np.arange(len(corr_array[0])), y=corr_array[0], mode='markers', marker_color='#1f77b4', marker_size=12, row=1, col=i+1)

            fig.add_scatter(x=np.arange(len(corr_array[0])), y=upper_y, mode='lines', line_color='rgba(255,255,255,0)', row=1, col=i+1)

            fig.add_scatter(x=np.arange(len(corr_array[0])), y=lower_y, mode='lines',fillcolor='rgba(32, 146, 230,0.3)',
                fill='tonexty', line_color='rgba(255,255,255,0)', row=1, col=i+1)

# 
            fig.update_traces(showlegend=False)
#             fig.update_xaxes(range=[-1,42])
            fig.update_yaxes(zerolinecolor='#000000')

        fig.update_layout(title= f'<b>[{col}] Autocorrelation (ACF)                                 [{col}] Partial Autocorrelation (PACF)<b>', 
                         title_x=0.5)
        fig.show()

# COMMAND ----------

autoCorrelationF1(data, 'average_usd')

# COMMAND ----------

# MAGIC %md
# MAGIC ### ìƒí˜¸êµì°¨ìƒê´€
# MAGIC - ì¹´í…Œê³ ë¦¬ê°€ ë„ˆë¬´ ë§ë‹¤. 4ê°œë§Œ êµì°¨í•´ì„œ ë³´ì collectible_avgusd, game_avgusd, all_avgusd, all_sales_usd
# MAGIC - ì¸ë±ìŠ¤265~315 (ì•½9ê°œì›”ì—ì„œ 10ê°œì›”ì£¼ê¸°)ê¹Œì§€ ë™í–‰ì„±ì´ ìˆìŒ

# COMMAND ----------

## ccf ì°¨íŠ¸ ìƒì„±ê¸°
def ccfcc_plot1(data):

    col_list = ['collectible_average_usd', 'game_average_usd','all_average_usd', 'all_sales_usd']
    xcol_list = []
    ycol_list = []
    ccfdata_list = []
    
    for i in range(len(col_list)-1):
        for j in range(1, len(col_list)):
            xcol_list.append(col_list[i])
            ycol_list.append(col_list[j])
            ccfdata_list.append(ccf(data[col_list[i]], data[col_list[j]], adjusted=False))
            
    plt.figure(figsize=(30,30), dpi=80)
    plt.suptitle("Cross Correlation Plot", fontsize=40)

    for i in range(len(ccfdata_list)):   
        ccfdata = ccfdata_list[i]
        # /* Compute the Significance level */
        nlags = len(ccfdata)
        conf_level = 2 / np.sqrt(nlags)

        # /* Draw Plot */
        plt.subplot(3, 3, i+1)   
        plt.title(f'<{xcol_list[i]} X {ycol_list[i]}, {min(np.where(ccfdata < 0)[0])-1} >', fontsize=22)
        plt.bar(x=np.arange(nlags), height=ccfdata, width=.3)
        plt.xlim(0,nlags)        

        plt.hlines(0, xmin=0, xmax=nlags, color='gray')  # 0 axis
        plt.hlines(conf_level, xmin=0, xmax=nlags, color='gray')
        plt.hlines(-conf_level, xmin=0, xmax=nlags, color='gray')
      
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

# COMMAND ----------

#  2~3ì—´ì´ ìƒí˜¸êµì°¨ìƒê´€ ê·¸ë˜í”„
ccfcc_plot1(data)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. CCF-LC ì‹œì°¨ ìƒê´€ê³„ìˆ˜(leads and lags correlation)
# MAGIC - ì‹œì°¨ ìƒí˜¸ ìƒê´€(TLCC) https://dive-into-ds.tistory.com/96
# MAGIC - ì„ í–‰ì (leading) / ë™í–‰ì (coincident) / í›„í–‰ì (lagging)

# COMMAND ----------

# # lag ë°ì´í„°í”„ë ˆì„ ìƒì„±ê¸°
# def lag_df(data, num):
#     col = data.columns
#     for i in range(1,num+1):
#         data[i] = data[col].shift(i)
#     return data

# COMMAND ----------

#  ì‹œì°¨ìƒê´€ê³„ìˆ˜ ê³„ì‚°í•¨ìˆ˜
def TLCC(X, Y, lag):
    result=[]
    print(lag)
    for i in range(lag):
        print(i)
        result.append(X.corr(Y.shift(i)))
        print(result)
    return np.round(result, 4)
#         print(i, np.round(result[i], 4))
#     print(f'ì‹œì°¨ìƒê´€ê³„ìˆ˜ê°€ ê°€ì¥ ë†’ì€ lag = <{np.argmax(result)}>')

# COMMAND ----------

TLCC(data['game_average_usd'], data['collectible_average_usd'], 14)

# COMMAND ----------

TLCC(data['all_average_usd'], data['all_sales_usd'], 14)

# COMMAND ----------

TLCC(data['all_average_usd'], data['all_number_of_sales'], 100)

# COMMAND ----------

# MAGIC %md
# MAGIC ### avg_usdí”¼ì²˜, ì¹´í…Œê³ ë¦¬ë³„ ì‹œì°¨ìƒê´€ë¶„ì„

# COMMAND ----------

# defiëŠ” 21-01-16ì— ë“¤ì–´ì˜´, ì´ 1704ì¤‘ã…‡ì— 400ê°œ, 1/6ë„ ì•ˆë˜ë¯€ë¡œ ì œì™¸í•œë‹¤
# data[['defi_average_usd']]['2021-01-15':]
avgusd_col_list = feature_classifier(data, 'average_usd')
avgusd_col_list.remove('defi_average_usd')
# avgusd_col_list.remove('all_average_usd')
print(len(avgusd_col_list), avgusd_col_list ) 

# COMMAND ----------

## TLCC ì°¨íŠ¸ ìƒì„±ê¸°
def TLCC_plot(data, col_list, nlag):

    xcol_list = []
    ycol_list = []
    TLCC_list = []

    for i in range(len(col_list)):
        for j in range(len(col_list)):
            if col_list[i] == col_list[j]:
                pass
            else:
                xcol_list.append(col_list[i])
                ycol_list.append(col_list[j])
                tlccdata =TLCC(data[col_list[i]], data[col_list[j]], nlag)
                TLCC_list.append(tlccdata)

    plt.figure(figsize=(30,40))
    plt.suptitle("TLCC Plot", fontsize=40)
    
    ncols = 3
    nrows = len(xcol_list)//3+1
    
    for i in range(len(TLCC_list)): 
        tlccdata = TLCC_list[i]
        plt.subplot(nrows, ncols, i+1)   
        plt.title(f'<{xcol_list[i]} X {ycol_list[i]}, {np.argmax(tlccdata)} >', fontsize=22)
        plt.plot(np.arange(len(tlccdata)), tlccdata)
        plt.xlim(-1,len(tlccdata)+1)        
        plt.vlines(np.argmax(tlccdata), ymin=min(tlccdata), ymax=max(tlccdata) , color='blue',linestyle='--',label='Peak synchrony')
#         plt.hlines(0, xmin=0, xmax=nlags, color='gray')  # 0 axis

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

# COMMAND ----------

# ê·¸ë˜í”„ ë„ˆë¬´ ë§ë‹¤. ë³´ê¸° í˜ë“œë‹ˆê¹Œ ìƒëµí•˜ì
# TLCC_plot(data, avgusd_col_list, 14)

# COMMAND ----------

## TLCC table ìƒì„±ê¸°
def TLCC_table(data, col_list, nlag):

    xcol_list = []
    ycol_list = []
    TLCC_list = []
    TLCC_max_idx_list = []
    TLCC_max_list = []
    havetomoreX = []
    havetomoreY = []
    result = []

    for i in range(len(col_list)):
        for j in range(len(col_list)):
            if col_list[i] == col_list[j]:
                pass
            else:
                xcol_list.append(col_list[i])
                ycol_list.append(col_list[j])
                tlccdata = TLCC(data[col_list[i]], data[col_list[j]], nlag)
                TLCC_list.append(tlccdata)
                
                TLCC_max_idx= np.argmax(tlccdata)
                TLCC_max_idx_list.append(TLCC_max_idx)
                if TLCC_max_idx == nlag-1:
                    havetomoreX.append(col_list[i])
                    havetomoreY.append(col_list[j])
    
                TLCC_max = max(tlccdata)
                TLCC_max_list.append(TLCC_max)
                if TLCC_max >= 0.9:
                    result.append('*****')  # ì•„ì£¼ ë†’ì€ ìƒê´€ê´€ê³„
                elif TLCC_max >= 0.7 and TLCC_max < 0.9: 
                    result.append('****')# ë†’ì€ ìƒê´€ê´€ê³„ê°€ ìˆìŒ
                elif TLCC_max >= 0.4 and TLCC_max < 0.7:
                    result.append('***')# ë‹¤ì†Œ ìƒê´€ê´€ê³„ê°€ ìˆìŒ
                elif TLCC_max >= 0.2 and TLCC_max < 0.4:
                    result.append('**')# ì•½í•œ ìƒê´€ê´€ê³„
                elif TLCC_max < 0.2:
                    result.append('*')# ìƒê´€ê´€ê³„ ê±°ì˜ ì—†ìŒ
                else :
                    print('ë¶„ê¸° ì²´í¬ í•„ìš”')
                    
    # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
    result_df = pd.DataFrame(data=list(zip(xcol_list, ycol_list, TLCC_max_idx_list, TLCC_max_list, result)), columns=['Lead(X)', 'Lag(Y)', 'TLCC_max_idx', 'TLCC_max', 'result'])
    
    # max_tlcc_idxê°€ ìµœëŒ€lagì™€ ë™ì¼í•œ ì¹¼ëŸ¼ ë°˜í™˜                
    return havetomoreX, havetomoreY, result_df

# COMMAND ----------

# gameì´ í›„í–‰ì¸ ê²½ìš°ëŠ” ëª¨ë‘ ê°€ì¥ ë†’ì€ lagê°€ ê°’ì´ ë†’ë‹¤. ë” ì˜¬ë ¤ë³´ì
# utilityëŠ” ë‹¤ë¥¸ì¹´í…Œê³ ë¦¬ì™€ ê±°ì˜ ì‹œì°¨ìƒê´€ì„±ì´ ì—†ë‹¤.
havetomoreX, havetomoreY, result_df = TLCC_table(data, avgusd_col_list, 14)
result_df

# COMMAND ----------

print(havetomoreX)
print(havetomoreY)

# COMMAND ----------

for i in range(len(havetomoreX)):
    tlccdata = TLCC(data[havetomoreX[i]], data[havetomoreY[i]], 150)
    print(havetomoreX[i], havetomoreY[i], np.argmax(tlccdata), np.round(max(tlccdata),4))

# COMMAND ----------

# ìµœëŒ€ lagê°’ìœ¼ë¡œ ë‹¤ì‹œ í™•ì¸í•´ë³´ì
havetomoreX, havetomoreY, result_df = TLCC_table(data, avgusd_col_list, 150)
result_df

# COMMAND ----------

# ì„ í–‰/í›„í–‰ì„ ìŒìœ¼ë¡œ ì¬ì •ë ¬í•˜ëŠ” í•¨ìˆ˜
def TLCC_table_filtered(data):
    result_xy_list = []
    result_after_x = []
    result_after_y = []
    for i in range(len(data)):
        result_xy_list.append(list(data.iloc[i, :2].values))

    for i in range(len(result_xy_list)):
        for j in range(len(result_xy_list)):
            if result_xy_list[i][0] == result_xy_list[j][1]  and result_xy_list[i][1] == result_xy_list[j][0]:
                result_after_x.append(result_xy_list[i][0])
                result_after_y.append(result_xy_list[i][1])
                result_after_x.append(result_xy_list[j][0])
                result_after_y.append(result_xy_list[j][1])


    result_XY_df = pd.DataFrame(data=list(zip(result_after_x, result_after_y)), columns=['after_X','after_Y']) # 'x->y, y->x ìŒë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    result_XY_df.drop_duplicates(inplace=True) # ì¤‘ë³µ ì œê±°
    result_XY_df.reset_index(inplace=True) # ì¸ë±ìŠ¤ ë¦¬ì…‹
    
    after_X = []
    after_Y = []
    TLCC_max_idx = []
    TLCC_max = []
    result = []
    print('<<TLCC ë°ì´í„°í”„ë ˆì„ì—ì„œ ìŒë³€ìˆ˜ìˆœìœ¼ë¡œ í•„í„°ë§>>')
    for i in range(len(result_XY_df)):
        xrow = data[data['Lead(X)']==result_XY_df['after_X'][i]]
        xyrow = xrow[xrow['Lag(Y)']==result_XY_df['after_Y'][i]]
        after_X.append(xyrow.values[0][0])
        after_Y.append(xyrow.values[0][1])
        TLCC_max_idx.append(xyrow.values[0][2])
        TLCC_max.append(xyrow.values[0][3])
        result.append(xyrow.values[0][4])

    result_df_filtered = pd.DataFrame(data=list(zip(after_X, after_Y, TLCC_max_idx, TLCC_max, result)), columns=['Lead(X)', 'Lag(Y)', 'TLCC_max_idx', 'TLCC_max', 'result'])
    return result_df_filtered

# COMMAND ----------

# ì¬ì •ë ¬ëœ ë°ì´í„°í”„ë ˆì„, ì´ 30ê°œ í–‰
result_df_filtered = TLCC_table_filtered(result_df)
print(len(result_df_filtered))
result_df_filtered

# COMMAND ----------

# ë†’ì€ ìƒê´€ê´€ê³„ë§Œ ì¶”ë ¤ë³´ì(0.5 ì´ìƒ) 20ê°œ
good = result_df_filtered[result_df_filtered['TLCC_max'] >= 0.5] #  0.7ì´ìƒì´ 18ê°œ
print(len(good))
good
# all->art(22), collectible/metaverse->all(54), all->game(44), art<->collectible(0), art->game(32), metaverse->art(99), collectible->game(58), meta->collec(95), meta->game(143)

# COMMAND ----------

# ë³´í†µ/ë‚®ì€ ìƒê´€ê´€ê³„ë§Œ ì¶”ë ¤ë³´ì(0.5 ì´í•˜) 10ê°œ
bad = result_df_filtered[result_df_filtered['TLCC_max'] <= 0.5]
print(len(bad))
bad

# COMMAND ----------

# ìµœê·¼ í•œë‹¬ ì¤‘ì•™ê°’
data[avgusd_col_list][-30:].median()

# COMMAND ----------

# MAGIC %md
# MAGIC #### [ì‹¤í—˜ ê²°ê³¼] avg_usd ì¹´í…Œê³ ë¦¬ë³„ ì‹œì°¨ìƒê´€ë¶„ì„
# MAGIC ### ìƒê´€ê´€ê³„ê°€ ë‚®ì€ ì¼€ì´ìŠ¤
# MAGIC ####  - utility
# MAGIC ---
# MAGIC ### ìƒê´€ê´€ê³„ê°€ ë†’ì€ ì¼€ì´ìŠ¤
# MAGIC ####  - ë™í–‰ : art-collectible/metaverse, collectible-metaverse, game-collectible
# MAGIC     - íŠ¹ì´ì‚¬í•­) art/collectible/metaverseëŠ” ëª¨ë‘ í‰ë‹¨ê°€ê°€ ë†’ì€ ì¹´í…Œê³ ë¦¬ì´ë‹¤. ì¶”ì •ìœ ì €êµ°ì¸ ì „ë¬¸íˆ¬ììë“¤ì€ ì¦‰ê° ë°˜ì‘ í•˜ë‚˜ë´„
# MAGIC       - artì‹œì¥ ê°€ê²©ê±°í’ˆì´ ë¹ ì§€ë©´ ë‹¤ë¥¸ ì‹œì¥ë„ ì˜í–¥ì„ ë°›ëŠ” ê²ƒì„
# MAGIC     - íŠ¹ì´ì‚¬í•­) game-collectibleì€ ìœ ì¼í•˜ê²Œ ì „ì²´ ìƒê´€ë¶„ì„ì—ë„ ë†’ì•˜ì—ˆëŠ”ë°..ì•„ë¬´ë˜ë„ ìœ ì €êµ°ì´ ê²¹ì¹˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •ë¨(ì´ìœ ëŠ” ì•„ë˜ ê³„ì†)
# MAGIC ####  - ì§€ì—° : art/collectible/metaverse-game(32,58,143), metaverse-collectible(95)
# MAGIC     - íŠ¹ì´ì‚¬í•­) gameì´ ì„ í–‰ì¸ ì§€ì—°ì¼€ì´ìŠ¤ëŠ” ì—†ìŒ, ì¦‰ ê²Œì„í‰ê· ê°€ëŠ” ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë¥¼ ë¦¬ë“œí•˜ì§€ ì•ŠëŠ”ë‹¤.
# MAGIC       - ìœ ì…/í™œë™ì´ ì œì¼ ë§ì•„ nftë§ˆì¼“ì˜ ë¹„ì¤‘ì€ ë†’ìœ¼ë‚˜ "ë§ˆì¼“ì˜ ê°€ê²©í˜•ì„±"ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ê²ƒìœ¼ë¡œ ë³´ì•„, ìœ ì € ì˜¤ë””ì–¸ìŠ¤ íŠ¹ì§•ì´ ë‹¤ë¥¸ ê²ƒìœ¼ë¡œ ì¶”ì •. ë¼ì´íŠ¸ìœ ì €(ê²Œì„í•˜ë©° ëˆë²Œê¸°) vs í—¤ë¹„ìœ ì €(ì „ë¬¸íˆ¬ìì)
# MAGIC       - íˆ¬ìê´€ì ì—ì„œ ê²Œì„ì¹´í…Œê³ ë¦¬ëŠ” íˆ¬ìí•´ë´¤ì ëˆì´ ì•ˆë˜ê² ë„¤..
# MAGIC       - ê²Œì„ë§Œ ë‹¤ë¥¸ì¹´í…Œê³ ë¦¬ì™€ ë¶„ëª…í•˜ê²Œ ë‹¤ë¥¸ ê²½í–¥ì„ ë³´ì´ëŠ” ì´ìœ 
# MAGIC         - í‰ê· ê°€ ë²”ìœ„ ê°­ì´ ë§¤ìš° í¼, ìµœê·¼ í•œë‹¬ ì¤‘ì•™ê°’ ê²Œì„ 193 vs 3514, 1384, 2402
# MAGIC         - game í‰ê· ê°€ëŠ” ì—„ì²­ ì‘ì€ë° íŒë§¤ìˆ˜ ë§¤ìš° ë§ì•„ì„œ ì‹œì¥ê°€ì¹˜(sales usd) ë¹„ì¤‘ì´ ê½¤ ë†’ìŒ, 22ë…„1ì›” ì¤‘ì•™ê°’ ê²Œì„ 25% vs 14.2%, 55.4%, 5.3% 
# MAGIC ---
# MAGIC ### ì˜ë¬¸ì 1 : ì™œ ê·¹ë‹¨ì ìœ¼ë¡œ ë™í–‰ì„±(0) vs 1~5ë‹¬ì§€ì—°(34-149)ìœ¼ë¡œ ë‚˜ë‰ ê¹Œ? 
# MAGIC ####  - ë°˜ì‘ì´ ë„ˆë¬´ ëŠë¦¬ë‹¤. ì¼ì •ê¸°ê°„ì´ ë„˜ìœ¼ë©´ ë¬´ì˜ë¯¸í•œê²ƒ ì•„ë‹ê¹Œ..? ê·¸ê²ƒì„ ì•Œê¸° ìœ„í•´, allì„ ê°™ì´ ë´ì•¼ê² ë‹¤.
# MAGIC     - ë™í–‰ : all-collectible, art/game-all
# MAGIC     - ì§€ì—° : all-art/game(22, 44), collectible/metaverse-all(54, 54),
# MAGIC       - ì˜ë¬¸ì ) allì€ í¬ê´„ì¸ë° ì™œ art/gameë³´ë‹¤ ì„ í–‰í•˜ë‚˜? ì¬ê·€ì  ì˜í–¥ê´€ê³„??
# MAGIC       - ì˜ë¬¸ì ) ì‹œì¥ê°€ì¹˜ ë¹„ì¤‘ 14%ì¸ artê°€ allê³¼ ë™í–‰í•˜ê³ , ë‚˜ë¨¸ì§€ 2ê°œëŠ” 54ì¼ì´ë‹¤. ì™œì¼ê¹Œ? ì™¸ë¶€ ìš”ì¸ì´ ìˆì„ ê²ƒìœ¼ë¡œ ì¶”ì •(ì–¸ë¡ ì´ìŠˆ)
# MAGIC     - ì¢…í•© : ì „ì²´ í‰ê· ê°€ì™€ ê°€ì¥ ë†’ì€ ì§€ì—°ì¸ 54ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì°¸ê³ í•  ìˆ˜ ìˆì„ê¹Œ? ì•„ë‹ˆë©´ ë§¤ìš° ê¸´ ì§€ì—°ë„ ìœ ì˜ë¯¸ í•˜ëŠ”ê±¸ê¹Œ?(ì¬ê·€ì  ì˜í–¥ê´€ê³„ë¡œ?) -
# MAGIC ---
# MAGIC ### ì˜ë¬¸ì 2 : ì„ í–‰/í›„í–‰ì˜ ê²°ê³¼ê°€ ê°™ê±°ë‚˜ ë‹¤ë¥¼ ê²½ìš° í•´ì„ì€ ì–´ë–»ê²Œ??
# MAGIC ####  - ìƒí˜¸ ë™í–‰ : ê±°ì˜ íŠ¹ì§•ì´ ë™ì¼í•˜ë‹¤ê³  ë´ë„ ë ë“¯
# MAGIC     - art<->collectible(0)
# MAGIC ####  - í¸ ì§€ì—°(í¸ë™í–‰ ìƒëµ) : aëŠ” bì— ë°”ë¡œ ë°˜ì‘ì´ ê°ˆ ì •ë„ë¡œ ì˜í–¥ì´ í¬ì§€ë§Œ, bëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë‚®ë‹¤?
# MAGIC     - metaverse -> art/collectible(99,55) -> game(32,58), meta->game(14),  collectible/metaverse->all(54), 3) all->art/game(22,44)
# MAGIC       - ì¸ê³¼ì— ë”°ë¼ì„œ ë©”íƒ€ë²„ìŠ¤ê°€ gameì— ì˜í–¥ì„ ì£¼ëŠ” ê±°ë¼ë©´ 143ì´ ìœ ì˜ë¯¸í•  ìˆ˜ë„ ìˆì„ ë“¯
# MAGIC       - allì´ art/gameì— ì¬ê·€ì ìœ¼ë¡œ ì˜í–¥ì„ ì£¼ëŠ” ê±°ë¼ë©´ allí”¼ì²˜ê°€ ìœ ì˜ë¯¸í•  ìˆ˜ë„ ìˆì„ ë“¯
# MAGIC ####  - ìƒí˜¸ ì§€ì—° : ì¦‰ì‹œ ë°˜ì‘ì„ ì¤„ì •ë„ì˜ ì˜í–¥ë ¥ì€ ì—†ëŠ”, ìƒëŒ€ì ìœ¼ë¡œ ì„œë¡œì—ê²Œ ë‚®ì€ ì˜í–¥ë ¥ì„ ê°€ì¡Œë‚˜?
# MAGIC     - ì—†ìŒ, ì´ ì¼€ì´ìŠ¤ê°€ í•©ë¦¬ì ì¸ ëª…ì œì¸ì§€ë„ ëª¨ë¥´ê² ìŒ í—·ê°ˆë¦¼
# MAGIC ---
# MAGIC ìœ„ ì˜ë¬¸ì„ í•´ì†Œí•˜ê¸° ìœ„í•œ ì¸ê³¼ê²€ì •ì´ í•„ìš”í•˜ë‹¤.
# MAGIC ---
# MAGIC #### ì¼€ì´ìŠ¤ ì…€ë ‰ì…˜
# MAGIC - ê³µì ë¶„ ê²€ì •ìš© ì¼€ì´ìŠ¤ : ì¼ë‹¨..ëŒ€í‘œ ì§€ì—°ì¼€ì´ìŠ¤ë¡œ collectible->game(59)ë¥¼ ê³µì ë¶„ ê²€ì¦í•´ë³´ì

# COMMAND ----------

# MAGIC %md
# MAGIC #### ëŒ€í‘œ ì¼€ì´ìŠ¤ ì‹œì°¨ìƒê´€ê³„ìˆ˜ ë¹„êµ í…Œì´ë¸”

# COMMAND ----------

avgusd_col_list

# COMMAND ----------

# ì›” ì¤‘ì•™ê°’ ì§‘ê³„ ë°ì´í„°
dataM_median = data.resample('M').median()
dataM_median.head()

# COMMAND ----------

# MAGIC %md
# MAGIC #### [í•¨ìˆ˜] ì‹œì°¨ìƒê´€ê³„ìˆ˜ ì°¨íŠ¸ ìƒì„±ê¸°

# COMMAND ----------

#  ì‹œì°¨ìƒê´€ê³„ìˆ˜ ê³„ì‚°í•¨ìˆ˜
def TLCC_comparison(X, Y, start_lag, end_lag):
    result=[]
    laglist = []
    for i in range(start_lag, end_lag+1):
        result.append(X.corr(Y.shift(i)))
        laglist.append(i)
    return laglist, np.round(result, 4)

# COMMAND ----------

# ì°¨íŠ¸ í•¨ìˆ˜
def TLCC_comparison_table(data, X, Y, startlag, endlag): # ë°ì´í„°, ê¸°ì¤€ë³€ìˆ˜, ë¹„êµë³€ìˆ˜, startlag, endlag
    Ylist = Y.copy()
    Ylist.remove(X)  # ì…ë ¥í•œ ë³€ìˆ˜ì—ì„œ ì‚­ì œë˜ê¸°ë•Œë¬¸ì— ì‚¬ì „ ì¹´í”¼í•„ìš”
    Yindex_list = [X, *Ylist]
    tlcc_list = []
    lag_var_list= []
    lvar_tlcc_list=[]
    sd_list = []
    rsd_list = []
    
    # yë³„ lag, tlccê°’ ë°›ì•„ì˜¤ê¸°
    for i in range(len(Yindex_list)): 
        ydata = data[Yindex_list[i]]
        lag_list,  result = TLCC_comparison(data[X], ydata, startlag, endlag) 
        tlcc_list.append(result)
        sd_list.append(np.std(ydata))   # =stdev(ë²”ìœ„)
        rsd_list.append(np.std(ydata)/np.mean(ydata)*100)  # stdev(ë²”ìœ„)/average(ë²”ìœ„)*100

#     # lagë³„ tlccê°’ ë°”ì¸ë”© ë³€ìˆ˜ ë§Œë“¤ê¸°(=ì¹¼ëŸ¼)
#     for i in range(len(lag_list)):
#         lag_var_list.append([]) #  lagë³„ tlccê°’ì„ ë°”ì¸ë”©í•  ê·¸ë¦‡ ìƒì„±
#         for j in range(len(tlcc_list)):
#              lag_var_list[i].append(tlcc_list[j][i])

    # ë°ì´í„°í”„ë ˆì„ìš© ë°ì´í„° ë§Œë“¤ê¸°
    temp = tlcc_list.copy()
    dfdata = list(zip(Yindex_list, sd_list, rsd_list, *list(zip(*temp)))) # temp..arrayë¥¼ zipí• ìˆ˜ ìˆë„ë¡ í’€ì–´ì¤˜ì•¼í•¨..
    
    # ë°ì´í„°í”„ë ˆì„ìš© ì¹¼ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ê¸°
    column_list = ['Yë³€ìˆ˜', 'í‘œì¤€í¸ì°¨', 'ìƒëŒ€í‘œì¤€í¸ì°¨', *lag_list]  

    result_df = pd.DataFrame(data=dfdata, columns= column_list,)
#     result_df = pd.DataFrame(data=dfdata, index = Yindex_list, columns= column_list)
#     result_df.index.name = f"X={X}" #  ì¸ë±ìŠ¤ ì´ë¦„ ë³€ê²½

    return result_df

# COMMAND ----------

# ì›” ì¤‘ì•™ê°’ ê¸°ì¤€      # collectibleì— ëŒ€í•œ êµì°¨ì‹œì°¨ìƒê´€ë¶„ì„
print(f"<<<Xê¸°ì¤€ Yì˜ ë³€ë™í­ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜ í…Œì´ë¸”>>>")
result_df = TLCC_comparison_table(dataM_median, 'collectible_average_usd', avgusd_col_list, -6, 6)
result_df

# COMMAND ----------

## ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼
# result_df.style.set_precision(2)
pd.set_option('display.precision', 2) # ì†Œìˆ˜ì  ê¸€ë¡œë²Œ ì„¤ì •
result_df.style.background_gradient(cmap='Blues').set_caption(f"<b><<<'X(0)ê¸°ì¤€ Yì˜ ë³€ë™í­ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜'>>><b>")
# df.style.applymap(lambda i: 'background-color: red' if i > 3 else '')

# COMMAND ----------

# gmaeì´ ìƒê°ë³´ë‹¤ ìƒê´€ì´ ë‚®ê²Œ ë‚˜ì™”ë‹¤. gameë°ì´í„°ëŠ” 2017ë…„ ë°ì´í„° ì—†ìœ¼ë¯€ë¡œ, 2018ë…„ ì´í›„ ë°ì´í„°ë¡œ ë‹¤ì‹œ í•´ë³´ì

# COMMAND ----------

# ì›” ì¤‘ì•™ê°’ ê¸°ì¤€ "2018ë…„ ì´í›„ (gameë°ì´í„°ëŠ” 2017ë…„ ë°ì´í„° ì—†ìŒ)"
print(f"<<<Xê¸°ì¤€ Yì˜ ë³€ë™í­ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜ í…Œì´ë¸”>>>")
result_df = TLCC_comparison_table(dataM_median['2018':], 'collectible_average_usd', avgusd_col_list, -6, 6)
result_df

# COMMAND ----------

## ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼ "2018ë…„ ì´í›„ (gameë°ì´í„°ëŠ” 2017ë…„ ë°ì´í„° ì—†ìŒ)"
# result_df.style.set_precision(2)
pd.set_option('display.precision', 2) # ì†Œìˆ˜ì  ê¸€ë¡œë²Œ ì„¤ì •
result_df.style.background_gradient(cmap='Blues').set_caption(f"<b><<<'X(0)ê¸°ì¤€ Yì˜ ë³€ë™í­ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜'>>><b>")
# df.style.applymap(lambda i: 'background-color: red' if i > 3 else '')

# COMMAND ----------

# MAGIC %md
# MAGIC #### [ê²°ë¡ ] ì›” ì¤‘ì•™ê°’ ê¸°ì¤€ ì‹œì°¨ìƒê´€ë¶„ì„(collectible_avgusd ê¸°ì¤€)
# MAGIC - 2018ë…„ì´í›„ ë°ì´í„°ë¡œ ë¶„ì„í•˜ë‹ˆ, ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìƒê´€ì„±ì´ ë†’ì•„ì¡Œë‹¤.(íŠ¹íˆ ê³¼ê±° ì‹œì°¨ê´€ë ¨)
# MAGIC - utilityëŠ” ìƒê´€ê´€ê³„ ì—†ìŒ
# MAGIC - metaverseëŠ” -lagê°€ ê´€ê³„ê°€ ìˆê³  +lagëŠ” ê´€ê³„ê°€ ë–¨ì–´ì§€ëŠ” ê²ƒìœ¼ë¡œ ë³´ì•„, meta -> collec ê´€ê³„ë¡œ ë³´ì„
# MAGIC - art, game ëª¨ë‘ +lagê´€ê³„ê°€ ë†’ì€ ê²ƒìœ¼ë¡œ ë³´ì•„, collec->metaê´€ê³„ë¡œ ë³´ì„, artëŠ” 6ê°œì›”ì°¨ì— ê´€ê³„ê°€ ë†’ì•„ì§
# MAGIC - í‘œì¤€í¸ì°¨/ ìƒëŒ€í‘œì¤€í¸ì°¨ ê°’ì´ ë„ˆë¬´ ì»¤ì„œ íŒë‹¨ì´ ì–´ë µë‹¤. í‰ê· ì„ í•¨ê»˜ ë´ì•¼í•˜ë‚˜?

# COMMAND ----------

# MAGIC %md
# MAGIC ### allì¹´í…Œê³ ë¦¬, í”¼ì²˜ë³„ ì‹œì°¨ìƒê´€ë¶„ì„

# COMMAND ----------

all_col_list = ['all_active_market_wallets','all_number_of_sales','all_average_usd','all_primary_sales','all_primary_sales_usd','all_sales_usd','all_secondary_sales','all_secondary_sales_usd','all_unique_buyers','all_unique_sellers']
print(len(all_col_list), all_col_list) # ì´ 10ê°œ ì¹´í…Œê³ ë¦¬

# COMMAND ----------

# ê·¸ë˜í”„ ë„ˆë¬´ ë§ë‹¤. ë³´ê¸° í˜ë“œë‹ˆê¹Œ ìƒëµí•˜ì
#  TLCC_plot(data, all_col_list, 14)

# COMMAND ----------

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

# COMMAND ----------

# avgusdê°€ í›„í–‰ì¸ê²½ìš° lagê°’ì´ ê°€ì¥ ë†’ë‹¤. ë” ì˜¬ë ¤ë³´ì
havetomoreX, havetomoreY, result_df = TLCC_table(data, all_col_list, 14)
result_df

# COMMAND ----------

print(havetomoreX)
print(havetomoreY)

# COMMAND ----------

for i in range(len(havetomoreX)):
    tlccdata = TLCC(data[havetomoreX[i]], data[havetomoreY[i]], 150)
    print(havetomoreX[i], havetomoreY[i], np.argmax(tlccdata), np.round(max(tlccdata),4))

# COMMAND ----------

# ìµœëŒ€ lagê°’ìœ¼ë¡œ ë‹¤ì‹œ í™•ì¸í•´ë³´ì
havetomoreX, havetomoreY, result_df = TLCC_table(data, all_col_list, 150)
result_df

# COMMAND ----------

# ì¬ì •ë ¬ëœ ë°ì´í„°í”„ë ˆì„, ì´ 90ê°œí–‰
result_df_filtered = TLCC_table_filtered(result_df)
print(len(result_df_filtered))
result_df_filtered

# COMMAND ----------

# ë†’ì€ ìƒê´€ê´€ê³„ë§Œ ì¶”ë ¤ë³´ì(0.75 ì´ìƒ) ì´ 93ê°œí–‰
good = result_df_filtered[result_df_filtered['TLCC_max'] >= 0.75]
print(len(good))
good
# ë™í–‰ì„±-ë™í–‰
# ì´ì§€ê°‘ìˆ˜<->ì´íŒë§¤ìˆ˜/1ì°¨íŒë§¤ìˆ˜/2ì°¨íŒë§¤ìˆ˜/2ì°¨ë§¤ì¶œ/êµ¬ë§¤ììˆ˜/íŒë§¤ììˆ˜,  ì´íŒë§¤ìˆ˜<->1ì°¨íŒë§¤ìˆ˜/2ì°¨íŒë§¤ìˆ˜/2ì°¨ë§¤ì¶œ/êµ¬ë§¤ììˆ˜/íŒë§¤ììˆ˜, 1ì°¨íŒë§¤ìˆ˜<->2ì°¨íŒë§¤ìˆ˜/2ì°¨ë§¤ì¶œ/êµ¬ë§¤ììˆ˜, ì´ë§¤ì¶œ<->2ì°¨ë§¤ì¶œ
# 2ì°¨íŒë§¤ìˆ˜<->êµ¬ë§¤ììˆ˜/íŒë§¤ììˆ˜, 2ì°¨ë§¤ì¶œ<->êµ¬ë§¤ììˆ˜, êµ¬ë§¤ììˆ˜<->íŒë§¤ììˆ˜

# ë™í–‰-ì§€ì—°
# ì´ì§€ê°‘ìˆ˜->ì´ë§¤ì¶œ(124), ì´íŒë§¤ìˆ˜->1ì°¨ë§¤ì¶œ(132)/ì´ë§¤ì¶œ(123), í‰ê· ê°€->1ì°¨ë§¤ì¶œ(98), ì´ë§¤ì¶œ->í‰ê· ê°€(98), 1ì°¨íŒë§¤ìˆ˜->1ì°¨ë§¤ì¶œ(119)/ì´ë§¤ì¶œ(117)/íŒë§¤ììˆ˜(143), ì´ë§¤ì¶œ->1ì°¨ë§¤ì¶œ(98), 2ì°¨ë§¤ì¶œ->1ì°¨ë§¤ì¶œ(118)
# 2ì°¨íŒë§¤ìˆ˜->ì´ë§¤ì¶œ(127), êµ¬ë§¤ììˆ˜->ì´ë§¤ì¶œ(123), íŒë§¤ììˆ˜->ì´ë§¤ì¶œ(130), 2ì°¨íŒë§¤ìˆ˜->2ì°¨ë§¤ì¶œ(125)
# íŒë§¤ììˆ˜->2ì°¨ë§¤ì¶œ(127)

# ì§€ì—°-ì§€ì—°
#  ì´ì§€ê°‘ìˆ˜<->í‰í‰ê· ê°€(100<->70),1ì°¨ë§¤ì¶œ(132<->56)  , ì´íŒë§¤ìˆ˜<->í‰ê· ê°€(100,66), í‰ê· ê°€<->1ì°¨íŒë§¤ìˆ˜(66,100),2ì°¨íŒë§¤ìˆ˜(65, 100),2ì°¨ë§¤ì¶œ(33,98),êµ¬ë§¤ììˆ˜(71,100),íŒë§¤ììˆ˜(67,100),  1ì°¨ë§¤ì¶œ<->2ì°¨íŒë§¤ìˆ˜(56,134)
# 1ì°¨ë§¤ì¶œ<->êµ¬ë§¤ììˆ˜(56,132),íŒë§¤ììˆ˜(56,135)

# COMMAND ----------

# ë³´í†µ/ë‚®ì€ ìƒê´€ê´€ê³„ë§Œ ì¶”ë ¤ë³´ì(0.75 ì´í•˜) ì—†ìŒ 7ê°œ
bad = result_df_filtered[result_df_filtered['TLCC_max'] <= 0.75]
print(len(bad))
bad

# COMMAND ----------

# MAGIC %md
# MAGIC #### [ì‹¤í—˜ ê²°ê³¼] allì¹´í…Œê³ ë¦¬ í”¼ì²˜ë³„ ì‹œì°¨ìƒê´€ë¶„ì„ 
# MAGIC 
# MAGIC ### ìƒê´€ê´€ê³„ê°€ ë‚®ì€ ì¼€ì´ìŠ¤
# MAGIC ####  - ëŒ€ì²´ë¡œ ë†’ë‹¤. 1ì°¨ë§¤ì¶œì´ ë¦¬ë“œí•˜ëŠ” ê²½ìš°ê°€ ìƒëŒ€ì ìœ¼ë¡œ 0.6~7ë¡œ ë‚®ì€í¸
# MAGIC ---
# MAGIC ### ìƒê´€ê´€ê³„ê°€ ë†’ì€ ì¼€ì´ìŠ¤
# MAGIC - í•­ëª©ì´ ë§ì•„ ë¶„ì„ì´ ì–´ë µë‹¤. êµ¬ì²´ì ì¸ ê³¼ì œì— ë§ì¶° ë¶„ì„í•˜ëŠ”ê²Œ ì¢‹ì„ ë“¯
# MAGIC - **í‰ê· ê°€ ê¸°ì¤€) ì´ì§€ê°‘ìˆ˜/ì´íŒë§¤ìˆ˜/1ì°¨íŒë§¤ìˆ˜/2ì°¨íŒë§¤ìˆ˜/2ì°¨ë§¤ì¶œ/êµ¬ë§¤ììˆ˜/íŒë§¤ììˆ˜ëŠ” í‰ê· ê°€ì™€ ëŒ€ì²´ë¡œ 2~3ë‹¬ì˜ ìƒí˜¸ì§€ì—°ê´€ê³„ì´ê³ , ì´ë§¤ì¶œê³¼ í‰ê· ê°€ ê·¸ë¦¬ê³  1ì°¨ë§¤ì¶œì€ ì•½ 3ë‹¬ì˜ í¸ì§€ì—°ê´€ê³„ë¥¼ ê°–ëŠ”ë‹¤.**
# MAGIC - **íŠ¹ì´ì‚¬í•­) ì‹œì°¨ì§€ì—°ì˜ ê²½ìš°, ìœ„ í‰ê· ê°€ í”¼ì²˜ë³„ë¶„ì„ì™€ ë™ì¼í•˜ê±°ë‚˜ ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ í¸ì´ê³ (33~143), "ìƒí˜¸ì§€ì—°ê´€ê³„" ë§ë‹¤.**
# MAGIC - **ì˜ë¬¸ì  ) "í‰ê· ê°€ì™€ êµ¬ë§¤ììˆ˜ì˜ ì§€ì—°ê´€ê³„ê°€ 2~3ë‹¬ì´ë©´ ìƒê°ë³´ë‹¤ ë„ˆë¬´ ê¸¸ë‹¤"**  
# MAGIC 
# MAGIC #### ìƒí˜¸ ë™í–‰
# MAGIC - ì´ì§€ê°‘ìˆ˜<->ì´íŒë§¤ìˆ˜/1ì°¨íŒë§¤ìˆ˜/2ì°¨íŒë§¤ìˆ˜/2ì°¨ë§¤ì¶œ/êµ¬ë§¤ììˆ˜/íŒë§¤ììˆ˜,  ì´íŒë§¤ìˆ˜<->1ì°¨íŒë§¤ìˆ˜/2ì°¨íŒë§¤ìˆ˜/2ì°¨ë§¤ì¶œ/êµ¬ë§¤ììˆ˜/íŒë§¤ììˆ˜, 
# MAGIC - 1ì°¨íŒë§¤ìˆ˜<->2ì°¨íŒë§¤ìˆ˜/2ì°¨ë§¤ì¶œ/êµ¬ë§¤ììˆ˜, ì´ë§¤ì¶œ<->2ì°¨ë§¤ì¶œ, 2ì°¨íŒë§¤ìˆ˜<->êµ¬ë§¤ììˆ˜/íŒë§¤ììˆ˜, 2ì°¨ë§¤ì¶œ<->êµ¬ë§¤ììˆ˜, êµ¬ë§¤ììˆ˜<->íŒë§¤ììˆ˜ 
# MAGIC 
# MAGIC #### í¸ ì§€ì—°(í¸ë™í–‰ ìƒëµ)
# MAGIC - ì´ì§€ê°‘ìˆ˜->ì´ë§¤ì¶œ(124), ì´íŒë§¤ìˆ˜->1ì°¨ë§¤ì¶œ(132)/ì´ë§¤ì¶œ(123), í‰ê· ê°€->1ì°¨ë§¤ì¶œ(98), ì´ë§¤ì¶œ->í‰ê· ê°€(98), 1ì°¨íŒë§¤ìˆ˜->1ì°¨ë§¤ì¶œ(119)/ì´ë§¤ì¶œ(117)/íŒë§¤ììˆ˜(143)
# MAGIC - ì´ë§¤ì¶œ->1ì°¨ë§¤ì¶œ(98), 2ì°¨ë§¤ì¶œ->1ì°¨ë§¤ì¶œ(118), 2ì°¨íŒë§¤ìˆ˜->ì´ë§¤ì¶œ(127), êµ¬ë§¤ììˆ˜->ì´ë§¤ì¶œ(123), íŒë§¤ììˆ˜->ì´ë§¤ì¶œ(130), 2ì°¨íŒë§¤ìˆ˜->2ì°¨ë§¤ì¶œ(125), íŒë§¤ììˆ˜->2ì°¨ë§¤ì¶œ(127)
# MAGIC 
# MAGIC #### ìƒí˜¸ ì§€ì—°
# MAGIC - ì´ì§€ê°‘ìˆ˜<->í‰ê· ê°€(100,70)/1ì°¨ë§¤ì¶œ(132,56), ì´íŒë§¤ìˆ˜<->í‰ê· ê°€(100,66), í‰ê· ê°€<->1ì°¨íŒë§¤ìˆ˜(66,100)/2ì°¨íŒë§¤ìˆ˜(65, 100)/2ì°¨ë§¤ì¶œ(33,98)/êµ¬ë§¤ììˆ˜(71,100)/íŒë§¤ììˆ˜(67,100)
# MAGIC - 1ì°¨ë§¤ì¶œ<->2ì°¨íŒë§¤ìˆ˜(56,134), 1ì°¨ë§¤ì¶œ<->êµ¬ë§¤ììˆ˜(56,132),íŒë§¤ììˆ˜(56,135)
# MAGIC 
# MAGIC ---
# MAGIC #### ì¼€ì´ìŠ¤ ì…€ë ‰ì…˜
# MAGIC - ê³µì ë¶„ ê²€ì •ìš© ì¼€ì´ìŠ¤ : ì¼ë‹¨..ëŒ€í‘œ ì§€ì—°ì¼€ì´ìŠ¤ë¡œ avgusd->buyer(71)ì„ ê³µì ë¶„ ê²€ì¦í•´ë³´ì(avg_usdë¥¼ ì˜ˆì¸¡í–ˆìœ¼ë‹ˆê¹Œ..)**

# COMMAND ----------

# MAGIC %md
# MAGIC #### ëŒ€í‘œ ì¼€ì´ìŠ¤ ì‹œì°¨ìƒê´€ê³„ìˆ˜ ë¹„êµ í…Œì´ë¸”

# COMMAND ----------

all_col_list

# COMMAND ----------

# ì›” ì¤‘ì•™ê°’ ì§‘ê³„ ë°ì´í„°
dataM_median.head()

# COMMAND ----------

# ì›” ì¤‘ì•™ê°’ ê¸°ì¤€
print(f"<<<Xê¸°ì¤€ Yì˜ ë³€ë™í­ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜ í…Œì´ë¸”>>>")
result_df = TLCC_comparison_table(dataM_median, 'all_average_usd', all_col_list, -6, 6)
result_df

# COMMAND ----------

## ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼
# result_df.style.set_precision(2) #ì•ˆë˜ë„¤..
pd.set_option('display.precision', 2) # ì†Œìˆ˜ì  ê¸€ë¡œë²Œ ì„¤ì •
# pd.set_option("styler.format.thousands", ",")#ì•ˆë˜ë„¤..
# result_df.style.format(thousands=",") # ì•ˆë¨
result_df.style.background_gradient(cmap='Blues').set_caption(f"<b><<<'X(0)ê¸°ì¤€ Yì˜ ë³€ë™í­ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜'>>><b>")

# df.style.applymap(lambda i: 'background-color: red' if i > 3 else '')

# COMMAND ----------

# MAGIC %md
# MAGIC #### [ê²°ë¡ ] ì›” ì¤‘ì•™ê°’ ê¸°ì¤€ ì‹œì°¨ìƒê´€ë¶„ì„(all_avgusd ê¸°ì¤€)
# MAGIC - ìê¸°ìƒê´€ : í•œë‹¬ ì „í›„ë§Œ ìˆìŒ
# MAGIC - ìƒí˜¸ì§€ì—°ê´€ê³„ : ì§€ê°‘ìˆ˜, íŒë§¤ìˆ˜, 1ì°¨íŒë§¤ìˆ˜, 2ì°¨íŒë§¤ìˆ˜, êµ¬ë§¤ììˆ˜, íŒë§¤ììˆ˜
# MAGIC - ìƒí˜¸ë™í–‰ê´€ê³„ : 1ì°¨ë§¤ì¶œ
# MAGIC - í¸ì§€ì—°ê´€ê³„ : ì´ë§¤ì¶œê³¼ 2ì°¨ë§¤ì¶œì´ í‰ê· ê°€ì— ì˜í–¥ì„ ì¤Œ
# MAGIC - í‘œì¤€í¸ì°¨/ ìƒëŒ€í‘œì¤€í¸ì°¨ ê°’ì´ ë„ˆë¬´ ì»¤ì„œ íŒë‹¨ì´ ì–´ë µë‹¤. í‰ê· ì„ í•¨ê»˜ ë´ì•¼í•˜ë‚˜?

# COMMAND ----------

# MAGIC %md
# MAGIC ### ì‹œê°í™”(pass)
# MAGIC - ì˜ˆì œ í•´ì„ì„ ëª»í•˜ê² ì–´ì„œ pass

# COMMAND ----------

# MAGIC %md 
# MAGIC #### ì˜ˆì œ1: line
# MAGIC - ì–´ë–»ê²Œ í•´ì„ì„ í•´ì•¼í• ì§€ ëª¨ë¥´ê² ë‹¤

# COMMAND ----------

def crosscorr(datax, datay, lag=0, wrap=False):
# """ Lag-N cross correlation. 
# Shifted data filled with NaNs 

# Parameters
# ----------
# lag : int, default 0
# datax, datay : pandas.Series objects of equal length
# wrap : NaN ì±„ìš°ëŠ” ê²ƒ. shift í•˜ë©´ì„œ ì‚¬ë¼ì§„ ê°’ìœ¼ë¡œ ë‹¤ì‹œ ì±„ìš°ê¸°. ê°’ì´ ìˆœí™˜ë˜ê²Œ ëœë‹¤. wrap=False ì´ë©´ NaNì€ dropí•˜ê³  correlation êµ¬í•œë‹¤.
# Returns
# ----------
# crosscorr : float
# """
    if wrap:
        shiftedy = datay.shift(lag)
        shiftedy.iloc[:lag] = datay.iloc[-lag:].values
        return datax.corr(shiftedy)
    else: 
        return datax.corr(datay.shift(lag))

# COMMAND ----------

#  í•´ì„ì–´ì¼€í•¨.. offset ì–‘ìˆ˜ì´ë¯€ë¡œ s2ê°€ ì„ í–‰í•œë‹¤? 59ì¼ì°¨?
s1 = data['game_average_usd']
s2 = data['collectible_average_usd']

rs = [crosscorr(s1,s2, lag) for lag in range(-300, 300)]
offset = np.floor(len(rs)/2)-np.argmax(rs) # ìµœëŒ€ correlation ê°’ ê°€ì§€ëŠ” offset ê³„ì‚°

f,ax=plt.subplots(figsize=(30,5))
# print(rs)
ax.plot(rs)
ax.axvline(np.ceil(len(rs)/2),color='k',linestyle='--',label='Center')
ax.axvline(np.argmax(rs),color='r',linestyle='--',label='Peak synchrony')
ax.set(title=f'Offset = {offset} \nS1 leads <> S2 leads', xlabel='Offset',ylabel='Pearson r')
# ax.set_xticks(range(-300, 300))
ax.set_xticklabels([-300, -150, -50, 0, 50, 150, 300])
plt.legend()

# Offsetì´ ì™¼ìª½ì— ìˆìœ¼ë©´, S1ì´ ë¦¬ë“œí•˜ê³¼ S2ê°€ ë”°ë¼ì˜¤ëŠ” ê²ƒ
# shift(-150)ì´ d2ì— ëŒ€í•´ì„œ ì ìš©ë˜ê³ , d2ì˜ ë¯¸ë˜ì™€ d1ì˜ í˜„ì¬ê°„ì— correlation ê³„ì‚° í•˜ëŠ” ê²ƒ. ì¦‰, offsetì´ ìŒìˆ˜ì´ë©´ d1ì´ ì„ í–‰í•œë‹¤ëŠ” ëœ»
# ì´ê²ƒë„ ê²°êµ­ global levelë¡œ correlation ì¸¡ì •í•˜ëŠ” ê²ƒ. ì‹œì°¨ ë‘ë©´ì„œ.

# COMMAND ----------

#  í•´ì„ì–´ì¼€í•¨.. offset ì–‘ìˆ˜ì´ë¯€ë¡œ s2ê°€ ì„ í–‰í•œë‹¤? ì´ê±´ ì´ìƒí•œë°.. í‰ê· ê°€ë³´ë‹¤ ë§ˆì¼“ì´ ì„ í–‰í•œë‹¤ê³ ?. ì„¸ì¼ì¦ˆë‘ ë¹„êµí•´ë´ì•¼í•˜ë‚˜.
s1 = data['all_average_usd']
s2 = data['all_sales_usd']

rs = [crosscorr(s1,s2, lag) for lag in range(-300, 300)]
offset = np.floor(len(rs)/2)-np.argmax(rs) # ìµœëŒ€ correlation ê°’ ê°€ì§€ëŠ” offset ê³„ì‚°

f,ax=plt.subplots(figsize=(30,5))
# print(rs)
ax.plot(rs)
ax.axvline(np.ceil(len(rs)/2),color='k',linestyle='--',label='Center')
ax.axvline(np.argmax(rs),color='r',linestyle='--',label='Peak synchrony')
ax.set(title=f'Offset = {offset} \nS1 leads <> S2 leads', xlabel='Offset',ylabel='Pearson r')
# ax.set_xticks(range(-300, 300))
ax.set_xticklabels([-300, -150, -50, 0, 50, 150, 300])
plt.legend()

# Offsetì´ ì™¼ìª½ì— ìˆìœ¼ë©´, S1ì´ ë¦¬ë“œí•˜ê³¼ S2ê°€ ë”°ë¼ì˜¤ëŠ” ê²ƒ
# shift(-150)ì´ d2ì— ëŒ€í•´ì„œ ì ìš©ë˜ê³ , d2ì˜ ë¯¸ë˜ì™€ d1ì˜ í˜„ì¬ê°„ì— correlation ê³„ì‚° í•˜ëŠ” ê²ƒ. ì¦‰, offsetì´ ìŒìˆ˜ì´ë©´ d1ì´ ì„ í–‰í•œë‹¤ëŠ” ëœ»
# ì´ê²ƒë„ ê²°êµ­ global levelë¡œ correlation ì¸¡ì •í•˜ëŠ” ê²ƒ. ì‹œì°¨ ë‘ë©´ì„œ.

# COMMAND ----------

#  í•´ì„ì–´ì¼€í•¨.. ëª¨ë¥´ê² ë‹¤.
s1 = data['all_average_usd']
s2 = data['all_number_of_sales']

rs = [crosscorr(s1,s2, lag) for lag in range(-300, 300)]
offset = np.floor(len(rs)/2)-np.argmax(rs) # ìµœëŒ€ correlation ê°’ ê°€ì§€ëŠ” offset ê³„ì‚°

f,ax=plt.subplots(figsize=(30,5))
# print(rs)
ax.plot(rs)
ax.axvline(np.ceil(len(rs)/2),color='k',linestyle='--',label='Center')
ax.axvline(np.argmax(rs),color='r',linestyle='--',label='Peak synchrony')
ax.set(title=f'Offset = {offset} \nS1 leads <> S2 leads', xlabel='Offset',ylabel='Pearson r')
# ax.set_xticks(range(-300, 300))
ax.set_xticklabels([-300, -150, -50, 0, 50, 150, 300])
plt.legend()

# Offsetì´ ì™¼ìª½ì— ìˆìœ¼ë©´, S1ì´ ë¦¬ë“œí•˜ê³¼ S2ê°€ ë”°ë¼ì˜¤ëŠ” ê²ƒ
# shift(-150)ì´ d2ì— ëŒ€í•´ì„œ ì ìš©ë˜ê³ , d2ì˜ ë¯¸ë˜ì™€ d1ì˜ í˜„ì¬ê°„ì— correlation ê³„ì‚° í•˜ëŠ” ê²ƒ. ì¦‰, offsetì´ ìŒìˆ˜ì´ë©´ d1ì´ ì„ í–‰í•œë‹¤ëŠ” ëœ»
# ì´ê²ƒë„ ê²°êµ­ global levelë¡œ correlation ì¸¡ì •í•˜ëŠ” ê²ƒ. ì‹œì°¨ ë‘ë©´ì„œ.

# COMMAND ----------

# MAGIC %md 
# MAGIC #### ì˜ˆì œ2: heatmap
# MAGIC - ì´ê²ƒë„ ë­˜ì–´ë–»ê²Œ ë´ì•¼í• ì§€ ëª¨ë¥´ê² ë‹¤.

# COMMAND ----------

data.shape[0]//20

# COMMAND ----------

no_splits = 30
samples_per_split = data.shape[0]//no_splits
rss=[]

for t in range(0, no_splits):
    d1 = data['game_average_usd'].iloc[(t)*samples_per_split:(t+1)*samples_per_split]
    d2 = data['collectible_average_usd'].iloc[(t)*samples_per_split:(t+1)*samples_per_split]
    rs = [crosscorr(d1,d2, lag) for lag in range(-300,300)]
    rss.append(rs)
rss = pd.DataFrame(rss)
f,ax = plt.subplots(figsize=(30,10))
sb.heatmap(rss, cmap='RdBu_r',ax=ax)
ax.set(title=f'Windowed Time Lagged Cross Correlation', xlabel='Offset',ylabel='Window epochs')
# ax.set_xticks([0, 50, 100, 151, 201, 251, 301])
# ax.set_xticklabels([-150, -100, -50, 0, 50, 100, 150]);

# Rolling window time lagged cross correlation
window_size = 300 #samples
t_start = 0
t_end = t_start + window_size
step_size = 30
rss=[]
while t_end < 1704:
    d1 = data['game_average_usd'].iloc[t_start:t_end]
    d2 = data['collectible_average_usd'].iloc[t_start:t_end]
    rs = [crosscorr(d1,d2, lag, wrap=False) for lag in range(-300,300)]
    rss.append(rs)
    t_start = t_start + step_size
    t_end = t_end + step_size
rss = pd.DataFrame(rss)

f,ax = plt.subplots(figsize=(30,10))
sb.heatmap(rss,cmap='RdBu_r',ax=ax)
ax.set(title=f'Rolling Windowed Time Lagged Cross Correlation',xlabel='Offset',ylabel='Epochs')
# ax.set_xticks([0, 50, 100, 151, 201, 251, 301])
# ax.set_xticklabels([-150, -100, -50, 0, 50, 100, 150]);

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. ê³µì ë¶„ ê²€ì •(Cointegration Test)
# MAGIC ####- ê°œë… : ì‹œê³„ì—´ Yì™€ ì‹œê³„ì—´ X ëª¨ë‘ <<ì¼ì°¨ ì°¨ë¶„ì´ ì•ˆì •ì ì¸ ì‹œê³„ì—´(I(1)ê³¼ì •, 1ì°¨ì ë¶„ê³¼ì •)ì´ê³ >> ë‘ì‹œê³„ì—´ ì‚¬ì´ì— ì•ˆì •ì ì¸ ì„ í˜•ê²°í•©ì´ ì¡´ì¬í•œë‹¤ë©´ ë‘ì‹œê³„ì—´ê°„ì— ê³µì ë¶„ì´ ì¡´ì¬í•œë‹¤ê³  ì •ì˜í•œë‹¤.
# MAGIC   - ì¦‰ Xì™€ Yê°€ ê³µì ë¶„ê´€ê³„ì— ìˆë‹¤ë©´ ì•„ë¬´ë¦¬ Xì™€ Yê°€ ë¶ˆì•ˆì •ì  í™•ë¥ ë³€ìˆ˜(I(1))ë¼ê³  í•´ë„ ë‘ë³€ìˆ˜ì— ëŒ€í•œ íšŒê·€ì‹ì„ ì„¸ì›Œë„ í—ˆêµ¬ì ì¸ íšŒê·€í˜„ìƒì´ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ”ë‹¤.
# MAGIC     - ê³µì ë¶„ ê´€ê³„ëŠ”, ë‹¨ê¸°ì ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ íŒ¨í„´ì„ ë³´ì´ì§€ë§Œ ì¥ê¸°ì ìœ¼ë¡œ ë³¼ ë•Œ ì¼ì •í•œ ê´€ê³„ê°€ ìˆìŒì„ ì˜ë¯¸í•¨, 
# MAGIC 
# MAGIC ####- ê²€ì • ë°©ë²• : ëŒ€í‘œì ìœ¼ë¡œ ìš”í•œìŠ¨ ê²€ì •ì„ ë§ì´ í•¨
# MAGIC   - engel & granget ê²€ì • : ADFë‹¨ìœ„ê·¼ê²€ì • ì•„ì´ë””ì–´
# MAGIC   - johansen ê²€ì • : ADFë‹¨ìœ„ê·¼ê²€ì •ì„ ë‹¤ë³€ëŸ‰ì˜ ê²½ìš°ë¡œ í™•ì¥í•˜ì—¬ ìµœìš°ì¶”ì •ì„ í†µí•´ ê²€ì • ìˆ˜í–‰

# COMMAND ----------

# MAGIC %md
# MAGIC ### (ë‹¨ë³€ëŸ‰)Engle-Granger 2step OLS Test
# MAGIC - statsmodels.coint ëŠ” engle-granger ê¸°ë°˜, [signatrue](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.coint.html)
# MAGIC - íšŒê·€ë¶„ì„ ê²°ê³¼ì˜ ì”ì°¨í•­ì— ëŒ€í•´ ê²€ì •
# MAGIC   - OLSì¶”ì •ëŸ‰ì„ ì´ìš©í•˜ì—¬ íšŒê·€ëª¨í˜• Y=bX+zì„ ì¶”ì •í•˜ì—¬ ì”ì°¨í•­ zhatì„ êµ¬í•œë‹¤. ê·¸ë¦¬ê³  ì”ì°¨í•­ zhatì— ëŒ€í•œ DFê²€ì •ì„ ìˆ˜í–‰í•œë‹¤.
# MAGIC   - ì¼ë°˜ DFì„ê³„ê°’ê³¼ëŠ” ë‹¤ë¥¸ ì„ê³„ê°‘ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.('ê³µì ë¶„ ê²€ì • ì„ê³„ê°’ ë¶„í¬í‘œ')
# MAGIC 
# MAGIC - ê·€ë¬´ê°€ì„¤ : ë¹„ì •ìƒ ì‹œê³„ì—´ ê°„ì˜ ì¡°í•©ì— ë”°ë¥¸ ì˜¤ì°¨í•­ì— ë‹¨ìœ„ê·¼ì´ ì¡´ì¬í•œë‹¤. ì¦‰, ì„œë¡œ ê³µì ë¶„ ê´€ê³„ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
# MAGIC   - p-valueê°’ì´ 5%ë³´ë‹¤ ì‘ì„ ë•Œ ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•˜ì—¬ ê³µì ë¶„ê´€ê³„ê°€ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
# MAGIC 
# MAGIC - ì•µê¸€&ê·¸ë ˆì¸ì € ê²€ì •ì˜ í•œê³„ë¡œ ì¼ë°˜ì ìœ¼ë¡œ ìš”í•œìŠ¨ì„ ë§ì´ ì‚¬ìš©í•œë‹¤.
# MAGIC   - ì‹œê³„ì—´ ì‚¬ì´ì— 1ê°œì˜ ê³µì ë¶„ ê´€ê³„ë§Œ íŒë³„í•  ìˆ˜ ìˆìŒ, ì¦‰3ê°œ ì´ìƒì˜ ì‹œê³„ì—´ì‚¬ì´ì˜ ê³µì ë¶„ ê²€ì • ë¶ˆê°€
# MAGIC   - íšŒê·€ ëª¨í˜•ìœ¼ë¡œ ì¥ê¸°ê· í˜•ê´€ê³„ë¥¼ íŒë‹¨í•  ë•Œ í‘œë³¸ì˜ í¬ê¸°ê°€ ë¬´í•œí•˜ì§€ ì•Šìœ¼ë©´ ì¢…ì†ë³€ìˆ˜ë¡œ ì–´ë–¤ ì‹œê³„ì—´ì„ ì„ íƒí•˜ëŠ”ì§€ì— ë”°ë¼ ê²€ì •ê²°ê³¼ê°€ ë‹¬ë¼ì§€ëŠ” ë¬¸ì œê°€ ìˆê³ , ì‹œê³„ì—´ ìˆ˜ê°€ ì¦ê°€í•˜ë©´ ë” ì‹¬í•´ì§
# MAGIC   
# MAGIC - [ì•µê¸€&ê·¸ë ˆì¸ì € ê³µì ë¶„ ê²€ì • ì˜ˆì œ1](https://mkjjo.github.io/finance/2019/01/25/pair_trading.html)
# MAGIC - [ì•µê¸€&ê·¸ë ˆì¸ì € ê³µì ë¶„ ê²€ì • ì˜ˆì œ2](https://lsjsj92.tistory.com/584)

# COMMAND ----------

import statsmodels.tsa.stattools as ts
# pd.set_option('display.precision', 2) 
# pd.options.display.float_format = '{:.2f}'.format

# COMMAND ----------


# ê³µì ë¶„ ê´€ê³„ ì‹œê°í™” (ë‘ë³€ìˆ˜ê°„ì˜ ë¹„ìœ¨ì´ í‰ê· ì„ ì¤‘ì‹¬ìœ¼ë¡œë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸) -> ì–´ë–»ê²Œ ë³´ëŠ”ê±°ì§€? ì¥ê¸°ì ìœ¼ë¡œ í¸ì°¨ê°€ ì ì–´ì§€ë©´ ì¥ê¸°ì  ê´€ê³„ê°€ ìˆë‹¤??
import statsmodels.tsa.stattools as ts
X = data['collectible_average_usd']['2018':]
Y = data['game_average_usd']['2018':]

# ë””í´íŠ¸ : rawë°ì´í„°(ë¡œê·¸ë³€í™˜/ìŠ¤ì¼€ì¼ë§ë“± ì •ê·œí™”í•˜ë©´ ì•ˆë¨, íŠ¹ì§• ì‚¬ë¼ì§), augmented engle&granger(default), maxlag(none), trend='c'
score, pvalue, _ = ts.coint(X,Y)
print('Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('ADF score: ' + str( np.round(score, 4) ))
print('Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ìƒìˆ˜&ê¸°ìš¸ê¸°')
score, pvalue, _ = ts.coint(X,Y, trend='ct')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ìƒìˆ˜&ê¸°ìš¸ê¸°(2ì°¨)')
score, pvalue, _ = ts.coint(X,Y, trend='ctt')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ì—†ìŒ')
score, pvalue, _ = ts.coint(X,Y, trend='nc')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))

(Y/X).plot(figsize=(30,10))
plt.axhline((Y/X).mean(), color='red', linestyle='--')
plt.xlabel('Time')
plt.title('collectible / game Ratio')
plt.legend(['collectible / game Ratio', 'Mean'])
plt.show()

# COMMAND ----------

# MAGIC %md
# MAGIC #### [EGê²°ê³¼] collectible avgusd vs game avgusd
# MAGIC - ì¶”ì„¸ ìƒìˆ˜&ê¸°ìš¸ê¸°(2ì°¨) ì¼€ì´ìŠ¤ : p-valueê°’ì´ 0.85ë¡œ 0.05ë¥¼ ì´ˆê³¼í•˜ì—¬ ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•˜ì—¬ **ê³µì ë¶„ê´€ê³„ ì—†ìŒ**
# MAGIC - ì¶”ì„¸ ì—†ìŒ ì¼€ì´ìŠ¤ : p-valueê°’ì´ 0.33ë¡œ 0.05ë¥¼ ì´ˆê³¼í•˜ì—¬ ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•˜ì—¬ **ê³µì ë¶„ê´€ê³„ ì—†ìŒ**

# COMMAND ----------

# ê³µì ë¶„ ê´€ê³„ ì‹œê°í™” -> ê´€ê³„ê°€ ìˆëŠ”ê±°ì•¼ë­ì•¼?
import statsmodels.tsa.stattools as ts
X = data['all_average_usd']
Y = data['all_unique_buyers']

# ë””í´íŠ¸ : rawë°ì´í„°(ë¡œê·¸ë³€í™˜/ìŠ¤ì¼€ì¼ë§ë“± ì •ê·œí™”í•˜ë©´ ì•ˆë¨, íŠ¹ì§• ì‚¬ë¼ì§), augmented engle&granger(default), maxlag(none), trend='c'
score, pvalue, _ = ts.coint(X,Y)
print('Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('ADF score: ' + str( np.round(score, 4) ))
print('Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ìƒìˆ˜&ê¸°ìš¸ê¸°')
score, pvalue, _ = ts.coint(X,Y, trend='ct')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ìƒìˆ˜&ê¸°ìš¸ê¸°(2ì°¨)')
score, pvalue, _ = ts.coint(X,Y, trend='ctt')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ì—†ìŒ')
score, pvalue, _ = ts.coint(X,Y, trend='nc')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))

(Y/X).plot(figsize=(30,10))
plt.axhline((Y/X).mean(), color='red', linestyle='--')
plt.xlabel('Time')
plt.title('ì´êµ¬ë§¤ììˆ˜/ì´í‰ê· ê°€ Ratio')
plt.legend(['ì´êµ¬ë§¤ììˆ˜/ì´í‰ê· ê°€ Ratio', 'Mean'])
plt.show()

# COMMAND ----------

# MAGIC %md
# MAGIC #### [EGê²°ê³¼] all_avgusd vs all_buyers
# MAGIC - ì¶”ì„¸ ìƒìˆ˜&ê¸°ìš¸ê¸°(2ì°¨) ì¼€ì´ìŠ¤ : p-valueê°’ì´ 0.55ë¡œ 0.05ë¥¼ ì´ˆê³¼í•˜ì—¬ ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•˜ì—¬ **ê³µì ë¶„ê´€ê³„ ì—†ìŒ**
# MAGIC - ì¶”ì„¸ ì—†ìŒ ì¼€ì´ìŠ¤ : p-valueê°’ì´ 0.13ë¡œ 0.05ë¥¼ ì´ˆê³¼í•˜ì—¬ ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•˜ì—¬ **ê³µì ë¶„ê´€ê³„ ì—†ìŒ**

# COMMAND ----------

# MAGIC %md
# MAGIC ### (ë‹¤ë³€ëŸ‰)Johansen Test
# MAGIC #### ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë¶„ì„ì— í¬í•¨, 
# MAGIC - VARëª¨í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì„¤ê²€ì •ì„ í†µí•´ ì ë¶„ê³„ì—´ê°„ ì•ˆì •ì ì¸ ì¥ê¸°ê· í˜•ê´€ê³„ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì ê²€í•˜ëŠ” ë°©ë²•
# MAGIC - 3ê°œ ì´ìƒì˜ ë¶ˆì•ˆì • ì‹œê³„ì—´ ì‚¬ì´ì˜ ê³µì ë¶„ ê²€ì •ì— í•œê³„ë¥¼ ê°–ëŠ” ì•µê¸€&ê·¸ë Œì € ê²€ì • ë°©ë²•ì„ ê°œì„ í•˜ì—¬ ë‹¤ë³€ëŸ‰ì—ë„ ê³µì ë¶„ ê²€ì •ì„ í•  ìˆ˜ ìˆìŒ
# MAGIC - statsmodels.tsa.vector_ar.vecm. coint_johansen 
# MAGIC   - VAR(VECM)ì˜ ê³µì ë¶„ ìˆœìœ„ë¥¼ ê²°ì •í•˜ê¸° ìœ„í•¨
# MAGIC   - [signature](https://www.statsmodels.org/dev/generated/statsmodels.tsa.vector_ar.vecm.coint_johansen.html)

# COMMAND ----------

#

# COMMAND ----------

from statsmodels.tsa.vector_ar.vecm import coint_johansen

# COMMAND ----------

X = data[avgusd_col_list]
X.head()

# COMMAND ----------

jresult = coint_johansen(X, det_order=0, k_ar_diff=1)
jresult.

# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. ê·¸ë ˆì¸ì € ì¸ê³¼ê²€ì •(Granger Causality)
# MAGIC - ê°œë… : ë™ì¼í•œ ì‹œê°„ì¶•ì˜ ë²”ìœ„ë¥¼ ê°€ì§„ ë‘ ë°ì´í„°ê°€ ìˆì„ ë•Œ í•œ ë°ì´í„°ë¥¼ ë‹¤ë¥¸ í•œìª½ì˜ ë°ì´í„°ì˜ íŠ¹ì •í•œ ì‹œê°„ê°„ê²©ì— ëŒ€í•´ì„œ ì„ í˜•íšŒê·€ë¥¼ í•  ìˆ˜ ìˆë‹¤ë©´ ê·¸ë˜ì¸ì € ì¸ê³¼ê°€ ìˆë‹¤ê³  í•˜ëŠ” ê²ƒì´ë‹¤.
# MAGIC   - A lags + B lagsë¡œ Bì˜ ë°ì´í„°ë¥¼ ì„ í˜•íšŒê·€í•œ ê²ƒì˜ ì˜ˆì¸¡ë ¥ > B lagsë¡œë§Œ Bì˜ ë°ì´í„°ë¥¼ ì„ í˜•íšŒê·€í•œ ê²ƒì˜ ì˜ˆì¸¡ë ¥
# MAGIC - ìœ ì˜ : ì¸ê³¼ì˜ ì˜¤í•´ì„ ê²½ê³„ í•„ìš”. (ì¸ê³¼ê´€ê³„ì˜ ì—¬ì§€ê°€ ìˆë‹¤ì •ë„ë¡œ í•´ì„)
# MAGIC   - ë‹¬ê±ì˜ ê°œì²´ìˆ˜ ì¦ê°€ì™€ ë¯¸ë˜ì˜ ë‹­ì˜ ê°œì²´ ìˆ˜ ì¦ê°€ì— ì¸ê³¼ ì˜í–¥ ê²°ê³¼ê°€ ìˆë‹¤ê³  í•´ì„œ ë°˜ë“œì‹œ ë‹­ì˜ ìˆ˜ì˜ ìš”ì¸ì€ ë‹¬ê±€ì˜ ê°œì²´ìˆ˜ë¼ê³  ë§í•˜ê¸°ì—” ë¬´ë¦¬ê°€ ìˆìŒ. ë‹¨ìˆœí•œ í™•ëŒ€í•´ì„ì´ê¸° ë•Œë¬¸, ê·¸ë˜ì„œ "ì¼ë°˜ì ì¸ ì¸ê³¼ê´€ê³„"ë¥¼ ë§í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¯€ë¡œ ì‚¬ëŒë“¤ì´ ìƒê°í•˜ëŠ” ì¶”ìƒì ì¸ ì¸ê³¼ê´€ê³„ë¥¼ ëª…í™•í•˜ê²Œ ë°í˜€ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë‹¤. 
# MAGIC   - ê·¸ë ˆì¸ì € ì¸ê³¼ê´€ê³„ëŠ” ìƒê´€ê´€ê³„ì²˜ëŸ¼ ê²°ê³¼ë¥¼ í•´ì„í•  ë•Œ ë…¼ë¦¬ì ìœ¼ë¡œ ê²°í•¨ì´ ì—†ëŠ”ì§€ ê³ ì°°í•˜ê³  í•´ì„í•  ë–„ ì£¼ì˜í•´ì•¼í•¨.
# MAGIC - **ì „ì œì¡°ê±´**
# MAGIC   - ì…ë ¥íŒŒë¼ë¯¸í„° : ì„ í–‰ì‹œê³„ì—´, í›„í–‰ì‹œê³„ì—´, ì‹œì°¨(ì§€ì—°)
# MAGIC   - ì‹œê³„ì—´ ë°ì´í„° ì •ìƒì„±
# MAGIC     - KPSSí…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ì •ìƒì„±ì„ ë§Œì¡±í•˜ëŠ” ì‹œì°¨ë¥¼ ì°¾ì•„ë‚¸ë‹¤.
# MAGIC     - 5.TSAì—ì„œ ë‹¨ìœ„ê·¼ê²€ì •ì„ í†µí•´ 1ì°¨ ì°¨ë¶„ì˜ ì •ìƒì„±ì„ í™•ì¸í–ˆìœ¼ë¯€ë¡œ ìƒëµí•œë‹¤.
# MAGIC   - í…ŒìŠ¤íŠ¸ ë°©í–¥ : ë³€ìˆ˜ A, Bì˜ ì–‘ë°©í–¥ìœ¼ë¡œ 2íšŒ ê²€ì • ì„¸íŠ¸ ìˆ˜í–‰ì´ ì¼ë°˜ì ì´ë©°, ê²°ê³¼ì— ë”°ë¼ í•´ì„ì´ ë‹¬ë¼ì§€ëŠ” ì–´ë ¤ì›€ì´ ìˆìŒ
# MAGIC - ê·€ë¬´ê°€ì„¤
# MAGIC   - ìœ ì˜ ìˆ˜ì¤€ì„ 0.05(5%)ë¡œ ì„¤ì •í•˜ì˜€ê³  í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ì„œ ê²€ì •ê°’(p-value)ê°€ 0.05ì´í•˜ë¡œ ë‚˜ì˜¤ë©´ ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•  ìˆ˜ ìˆë‹¤. ê·€ë¬´ê°€ì„¤ì€ â€œGranger Causalityë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤â€ ì´ë‹¤.
# MAGIC - [í´ë¼ì´ë¸Œ ê·¸ë ˆì¸ì € ìœ„í‚¤](https://ko.wikipedia.org/wiki/%ED%81%B4%EB%9D%BC%EC%9D%B4%EB%B8%8C_%EA%B7%B8%EB%A0%88%EC%9D%B8%EC%A0%80)
# MAGIC - [ê·¸ë ˆì¸ì € ì¸ê³¼ê´€ê³„](https://intothedata.com/02.scholar_category/timeseries_analysis/granger_causality/)

# COMMAND ----------

# MAGIC %md
# MAGIC #### ì •ìƒì„± ì‹œì°¨ ì°¾ê¸°
# MAGIC - í†µê³„ì  ê°€ì„¤ ê²€ì •(Unit root test:ë‹¨ìœ„ê·¼ê²€ì •)
# MAGIC - ë‹¨ìœ„ê·¼ : ë‹¨ìœ„ê·¼ì´ë€ í™•ë¥ ë¡ ì˜ ë°ì´í„° ê²€ì •ì—ì„œ ì“°ì´ëŠ” ê°œë…ìœ¼ë¡œ ì‹œê³„ì—´ ë°ì´í„°ëŠ” ì‹œê°„ì— ë”°ë¼ ì¼ì •í•œ ê·œì¹™ì„ ê°€ì§ì„ ê°€ì •í•œë‹¤
# MAGIC 
# MAGIC #### 1. Augmented Dickey-Fuller("ADF") Test
# MAGIC - ì‹œê³„ì—´ì— ë‹¨ìœ„ê·¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì •,ë‹¨ìœ„ê·¼ì´ ì¡´ì¬í•˜ë©´ ì •ìƒì„± ì‹œê³„ì—´ì´ ì•„ë‹˜.
# MAGIC - ê·€ë¬´ê°€ì„¤ì´ ë‹¨ìœ„ê·¼ì´ ì¡´ì¬í•œë‹¤.
# MAGIC - ê²€ì¦ ì¡°ê±´ ( p-value : 5%ì´ë‚´ë©´ rejectìœ¼ë¡œ ëŒ€ì²´ê°€ì„¤ ì„ íƒë¨ )
# MAGIC - ê·€ë¬´ê°€ì„¤(H0): non-stationary. ëŒ€ì²´ê°€ì„¤ (H1): stationary.
# MAGIC - adf ì‘ì„ ìˆ˜ë¡ ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°ì‹œí‚¬ í™•ë¥ ì´ ë†’ë‹¤
# MAGIC #### 2. Kwiatkowski-Phillips-Schmidt-Shin (â€œKPSSâ€) Test
# MAGIC - KPSS ê²€ì •ì€ 1ì¢… ì˜¤ë¥˜ì˜ ë°œìƒê°€ëŠ¥ì„±ì„ ì œê±°í•œ ë‹¨ìœ„ê·¼ ê²€ì • ë°©ë²•ì´ë‹¤.
# MAGIC - DF ê²€ì •, ADF ê²€ì •ê³¼ PP ê²€ì •ì˜ ê·€ë¬´ê°€ì„¤ì€ ë‹¨ìœ„ê·¼ì´ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì´ë‚˜, KPSS ê²€ì •ì˜ ê·€ë¬´ê°€ì„¤ì€ ì •ìƒ ê³¼ì • (stationary process)ìœ¼ë¡œ ê²€ì • ê²°ê³¼ì˜ í•´ì„ ì‹œ ìœ ì˜í•  í•„ìš”ê°€ ìˆë‹¤.
# MAGIC   - ê·€ë¬´ê°€ì„¤ì´ ë‹¨ìœ„ê·¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
# MAGIC - ë‹¨ìœ„ê·¼ ê²€ì •ê³¼ ì •ìƒì„± ê²€ì •ì„ ëª¨ë‘ ìˆ˜í–‰í•¨ìœ¼ë¡œì„œ ì •ìƒ ì‹œê³„ì—´, ë‹¨ìœ„ê·¼ ì‹œê³„ì—´, ë˜ í™•ì‹¤íˆ ì‹ë³„í•˜ê¸° ì–´ë ¤ìš´ ì‹œê³„ì—´ì„ êµ¬ë¶„í•˜ì˜€ë‹¤.
# MAGIC - KPSS ê²€ì •ì€ ë‹¨ìœ„ê·¼ì˜ ë¶€ì¬ê°€ ì •ìƒì„± ì—¬ë¶€ì— ëŒ€í•œ ê·¼ê±°ê°€ ë˜ì§€ ëª»í•˜ë©° ëŒ€ë¦½ê°€ì„¤ì´ ì±„íƒë˜ë©´ ê·¸ ì‹œê³„ì—´ì€ trend-stationarity(ì¶”ì„¸ë¥¼ ì œê±°í•˜ë©´ ì •ìƒì„±ì´ ë˜ëŠ” ì‹œê³„ì—´)ì„ ê°€ì§„ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# MAGIC - ë•Œë¬¸ì— KPSS ê²€ì •ì€ ë‹¨ìœ„ê·¼ì„ ê°€ì§€ì§€ ì•Šê³  Trend- stationaryì¸ ì‹œê³„ì—´ì€ ë¹„ì •ìƒ ì‹œê³„ì—´ì´ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# COMMAND ----------

#  ì‹œì°¨ìƒê´€ê³„ìˆ˜ ê³„ì‚°í•¨ìˆ˜ (cointëŠ” ì•µê¸€&ê·¸ë ˆì¸ì € ê¸°ë°˜ìœ¼ë¡œ pvalueëŠ” adfë‹¨ìœ„ê·¼ ê²€ì •)
def adf_lag(X, Y, start_lag, end_lag):
    pvalue=[]
    for i in range(start_lag, end_lag+1):
        _, p, _ = ts.coint(X,Y.shift(i, fill_value=0))
        pvalue.append(p)
    return pvalue

def kpss_lag(X, Y, start_lag, end_lag):
    pvalue=[]
    for i in range(start_lag, end_lag+1):
        stats, p, lag, _ = kpss(X, regression="ct", nlags=i)
        pvalue.append(p)
    return pvalue

# COMMAND ----------

# MAGIC %md
# MAGIC ##### collectible_avgusd & game_avgusd
# MAGIC - ê·¸ë ˆì¸ì €ì¸ê³¼ê²€ì •ê³¼ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ lagë¥¼ 15ë¡œ ì¡ì
# MAGIC - KPSSê¸°ì¤€ ìµœëŒ€ 11ê°œì›”ê¹Œì§€ ìƒí˜¸ì§€ì—°ê´€ê³„ "ì •ìƒì„±" ìˆìŒ, cg 11, gc12

# COMMAND ----------

xcol = 'collectible_average_usd'
ycol = 'game_average_usd'
X = dataM_median[xcol]['2018':]
Y = dataM_median[ycol]['2018':]

startlag = -15
endlag = 15
# adf_pval = adf_lag(X,Y, startlag, endlag)
# kpss_pval = kpss_lag(X,Y, startlag, endlag)

fig = plt.figure(figsize=(30,10))
plt.suptitle("lag difference sationary check <ADF & KPSS>", fontsize=30)

plt.subplot(2, 1, 1)   
plt.title('<ADF pvalue>', fontsize=22)
plt.plot(range(startlag, endlag+1), adf_lag(X,Y, startlag, endlag), label = f'{xcol} -> {ycol}')
plt.plot(range(startlag, endlag+1), adf_lag(Y,X, startlag, endlag), label = f'{ycol} -> {xcol}')
plt.legend(loc='center left')
plt.hlines(0.05, xmin=startlag, xmax=endlag, color='blue')

plt.subplot(2, 1, 2)
plt.title('<KPSS pvalue>', fontsize=22)
plt.plot(range(startlag, endlag+1), kpss_lag(X,Y, startlag, endlag), label = f'{xcol} -> {ycol}')
plt.plot(range(startlag, endlag+1), kpss_lag(Y,X, startlag, endlag), label = f'{ycol} -> {xcol}')
plt.legend(loc='center left')
plt.hlines(0.05, xmin=startlag, xmax=endlag, color='blue')

plt.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ##### all_avgusd & all_buyers
# MAGIC - ê·¸ë ˆì¸ì €ì¸ê³¼ê²€ì •ê³¼ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ lagë¥¼ 15ë¡œ ì¡ì
# MAGIC - KPSSê¸°ì¤€ ìµœëŒ€ 12ê°œì›”ê¹Œì§€ ìƒí˜¸ì§€ì—°ê´€ê³„ "ì •ìƒì„±" ìˆìŒ, ub12, bu15

# COMMAND ----------

xcol = 'all_average_usd'
ycol = 'all_unique_buyers'
X = dataM_median[xcol]
Y = dataM_median[ycol]

startlag = -15
endlag = 15
# adf_pval = adf_lag(X,Y, startlag, endlag)
# kpss_pval = kpss_lag(X,Y, startlag, endlag)

fig = plt.figure(figsize=(30,10))
plt.suptitle("lag difference sationary check <ADF & KPSS>", fontsize=30)

plt.subplot(2, 1, 1)   
plt.title('<ADF pvalue>', fontsize=22)
plt.plot(range(startlag, endlag+1), adf_lag(X,Y, startlag, endlag), label = f'{xcol} -> {ycol}')
plt.plot(range(startlag, endlag+1), adf_lag(Y,X, startlag, endlag), label = f'{ycol} -> {xcol}')
plt.legend(loc='center left')
plt.hlines(0.05, xmin=startlag, xmax=endlag, color='blue')

plt.subplot(2, 1, 2)
plt.title('<KPSS pvalue>', fontsize=22)
plt.plot(range(startlag, endlag+1), kpss_lag(X,Y, startlag, endlag), label = f'{xcol} -> {ycol}')
plt.plot(range(startlag, endlag+1), kpss_lag(Y,X, startlag, endlag), label = f'{ycol} -> {xcol}')
plt.legend(loc='center left')
plt.hlines(0.05, xmin=startlag, xmax=endlag, color='blue')

plt.show()

# COMMAND ----------

# MAGIC %md
# MAGIC #### ê·¸ë ˆì¸ì € ì¸ê³¼ë¶„ì„
# MAGIC - ë”•ì…”ë„ˆë¦¬ ì–¸íŒ¨í‚¹ì„ ëª»í•´ì„œ ì‹œê°í™”ëª»í•¨
# MAGIC - **ì •ìƒì„±ì‹œì°¨ : ìµœëŒ€ cg11, gc12 ub12, bu15**
# MAGIC - from statsmodels.tsa.stattools import grangercausalitytests [signature](https://www.statsmodels.org/dev/generated/statsmodels.tsa.stattools.grangercausalitytests.html)
# MAGIC   - 2ê°œ ì‹œê³„ì—´ì˜ ê·¸ëœì € ë¹„ì¸ê³¼ì„±ì— ëŒ€í•œ 4ê°€ì§€ í…ŒìŠ¤íŠ¸.
# MAGIC   - 2ë²ˆì§¸ ì‹œê³„ì—´ì´ 1ë²ˆì§¸ ì‹œê³„ì—´ì„ ìœ ë°œí•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸(2->1)
# MAGIC   - maxlag = 15ê°€ ìµœëŒ€

# COMMAND ----------

from statsmodels.tsa.stattools import grangercausalitytests

# COMMAND ----------

# collectible -> game, 6~15ê¹Œì§€ ê·€ë¬´ê°€ì„¤ ê¸°ê°í•˜ì—¬ collectibleë¡œ gameì„ ì˜ˆì¸¡ í•  ìˆ˜ ìˆìŒ
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(data[['game_average_usd', 'collectible_average_usd']]['2018':], maxlag=15)

# COMMAND ----------

# collectible -> game, 6~15ê¹Œì§€ ê·€ë¬´ê°€ì„¤ ê¸°ê°í•˜ì—¬ collectibleë¡œ gameì„ ì˜ˆì¸¡ í•  ìˆ˜ ìˆìŒ
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(dataM_median[['game_average_usd', 'collectible_average_usd']]['2018':], maxlag=15)

# COMMAND ----------

#  game -> collectible, 1~10ê¹Œì§€ ê·€ë¬´ê°€ì„¤ê¸°ê°í•˜ì—¬ gameìœ¼ë¡œ collectibleì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŒ
grangercausalitytests(dataM_median[['collectible_average_usd', 'game_average_usd']]['2018':], maxlag=15)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### collectible_avgusd & game_avgusd
# MAGIC - ***ì •ìƒì„± ì‹œì°¨ : ìµœëŒ€ cg11, gc12***
# MAGIC - ***ê·¸ë ˆì¸ì €ì¸ê³¼ ì‹œì°¨ : cg 6 ~ 15, gc1 ~ 10***
# MAGIC - ***pvalue ê¸°ì¤€***
# MAGIC   - collectibleì´ gameì„ 6 ~ 11ê°œì›” ì„ í–‰í•œë‹¤. 
# MAGIC   - gameì´ collectibleì„ 1 ~ 10ê°œì›” ì„ í–‰í•œë‹¤. 
# MAGIC - ***f stats ê¸°ì¤€ : gcê°€ ë” ë†’ìœ¼ë¯€ë¡œ, cê°€ ë¨¼ì € gë¥¼ ë¦¬ë“œí•˜ê³  ì´í›„ ë°˜ëŒ€ë¡œ ë‹¤ì‹œ ì˜í–¥ì„ ë°›ëŠ”ë‹¤.***
# MAGIC   - c -> g, lag 6, 4.3468  
# MAGIC   - g -> c, lag 6, 39.8356
# MAGIC ---
# MAGIC - ***ì¢…í•© í•´ì„***
# MAGIC   - ìƒí˜¸ì¸ê³¼ê´€ê³„ì´ë‚˜ gê°€ cì—ê²Œ ë” ë¹ ë¥¸ ì˜í–¥ë¥¼ ì¤€ë‹¤.(1ë‹¬ë§Œì—)
# MAGIC   - ê·¸ëŸ¬ë‚˜ ìƒí˜¸ì¸ê³¼ê´€ê³„ê°€ ìˆëŠ” 6ê°œì›” ê¸°ì¤€ìœ¼ë¡œ ë³´ì•˜ì„ ë•Œ, cê°€ gë¥¼ ë” ë¦¬ë“œí•œë‹¤.
# MAGIC   - ìƒí˜¸ì¸ê³¼ê´€ê³„ê°€ ì„±ë¦½ë˜ë¯€ë¡œ ì œ 3ì˜ ì™¸ë¶€ ë³€ìˆ˜ ì˜í–¥ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.(ex ì™¸ë¶€ì–¸ë¡ , í™ë³´ ë“±), ì´ ê²½ìš° varëª¨í˜•ì„ ì‚¬ìš©í•´ì•¼í•œë‹¤.

# COMMAND ----------

# buyer -> avgusd, 2~15ê¹Œì§€ ê·€ë¬´ê°€ì„¤ ê¸°ê°í•˜ì—¬ buyerë¡œ avgusdë¥¼ ì˜ˆì¸¡ í•  ìˆ˜ ìˆìŒ
grangercausalitytests(dataM_median[['all_average_usd', 'all_unique_buyers']], maxlag=15)

# COMMAND ----------

# avgusd -> buyer, 1~15ê¹Œì§€ ê·€ë¬´ê°€ì„¤ ê¸°ê°í•˜ì—¬ avgusdë¡œ buyerë¥¼ ì˜ˆì¸¡ í•  ìˆ˜ ìˆìŒ
grangercausalitytests(dataM_median[['all_unique_buyers', 'all_average_usd']], maxlag=15)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### all_avgusd & all_buyers
# MAGIC - **ì •ìƒì„± ì‹œì°¨ : ìµœëŒ€ ub 12, bu 15**
# MAGIC - **ê·¸ë ˆì¸ì €ì¸ê³¼ ì‹œì°¨ : ub1 ~ 15 , bu 2 ~ 15**
# MAGIC - **pvalue ê¸°ì¤€**
# MAGIC   - avgusdê°€  buyerë¥¼ 1 ~ 12ê°œì›” ì„ í–‰í•œë‹¤. 
# MAGIC   - buyerê°€ avgusdë¥¼ 2 ~ 15ê°œì›” ì„ í–‰í•œë‹¤.
# MAGIC 
# MAGIC - **f stats ê¸°ì¤€ : buê°€ ë” ë†’ìœ¼ë¯€ë¡œ, uê°€ ë¨¼ì € bë¥¼ ë¦¬ë“œí•˜ê³  ì´í›„ ë°˜ëŒ€ë¡œ ë‹¤ì‹œ ì˜í–¥ì„ ë°›ëŠ”ë‹¤.**
# MAGIC   - u -> b, lag 2, 40.0170 
# MAGIC   - b -> u, lag 2, 59.8666
# MAGIC ---
# MAGIC - **ì¢…í•© í•´ì„**
# MAGIC   - bëŠ” ê±°ì˜ ë™í–‰ì„±ì„ ë³´ì¸ë‹¤.
# MAGIC   - ìƒí˜¸ì¸ê³¼ê´€ê³„ì´ë‚˜, uê°€ bì—ê²Œ ë” ë¹ ë¥¸ ì˜í–¥ë¥¼ ì¤€ë‹¤.(1ë‹¬ë§Œì—, ê·¼ë° ë¹„ìŠ·í•¨)
# MAGIC   - ê·¸ëŸ¬ë‚˜ ìƒí˜¸ì¸ê³¼ê´€ê³„ê°€ ìˆëŠ” 2ê°œì›” ê¸°ì¤€ìœ¼ë¡œ ë³´ì•˜ì„ ë•Œ, uê°€ bë¥¼ ë” ë¦¬ë“œí•œë‹¤.
# MAGIC   - ìƒí˜¸ì¸ê³¼ê´€ê³„ê°€ ì„±ë¦½ë˜ë¯€ë¡œ ì œ 3ì˜ ì™¸ë¶€ ë³€ìˆ˜ ì˜í–¥ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.(ex ì™¸ë¶€ì–¸ë¡ , í™ë³´ ë“±), ì´ ê²½ìš° varëª¨í˜•ì„ ì‚¬ìš©í•´ì•¼í•œë‹¤.

# COMMAND ----------

# collectible -> all, 2~13 ê·€ë¬´ê°€ì„¤ ê¸°ê°
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(dataM_median[['all_average_usd', 'collectible_average_usd']]['2018':], maxlag=15)

# COMMAND ----------

# all -> collectible, 3~11 ê·€ë¬´ê°€ì„¤ ê¸°ê°
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(dataM_median[['collectible_average_usd', 'all_average_usd']]['2018':], maxlag=15)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### all_avgusd & collectible_avgusd
# MAGIC - all -> collectible : 3~11 ê·€ë¬´ê°€ì„¤ ê¸°ê°, 3ê¸°ì¤€ fstats 16.1708
# MAGIC - collectible -> all : 2~13 ê·€ë¬´ê°€ì„¤ ê¸°ê°, 3ê¸°ì¤€ fstats 75.9002

# COMMAND ----------

# collectible -> buyers 1~15 ê·€ë¬´ê°€ì„¤ ê¸°ê°
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(dataM_median[['all_unique_buyers', 'collectible_average_usd']]['2018':], maxlag=15)

# COMMAND ----------

# buyers -> collectible 5~11 ê·€ë¬´ê°€ì„¤ ê¸°ê°
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(dataM_median[['collectible_average_usd', 'all_unique_buyers']]['2018':], maxlag=15)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### all_buyers & collectible_avgusd
# MAGIC - buyers -> collectible 5~11 ê·€ë¬´ê°€ì„¤ ê¸°ê°, 5ê¸°ì¤€ fstats 13.7463
# MAGIC - collectible -> buyers 1~15 ê·€ë¬´ê°€ì„¤ ê¸°ê°, 5ê¸°ì¤€ fstats 35.7845

# COMMAND ----------

# game -> all 1~8, 15 ê·€ë¬´ê°€ì„¤ ê¸°ê°
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(dataM_median[['all_average_usd', 'game_average_usd']]['2018':], maxlag=15)

# COMMAND ----------

# all -> game 5~15 ê·€ë¬´ê°€ì„¤ ê¸°ê°
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(dataM_median[['game_average_usd', 'all_average_usd']]['2018':], maxlag=15)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### all_avgusd & game_avgusd
# MAGIC - all -> game 5~15 ê·€ë¬´ê°€ì„¤ ê¸°ê°, 5ê¸°ì¤€ fstats 16.0765
# MAGIC - game -> all 1~8, 15 ê·€ë¬´ê°€ì„¤ ê¸°ê°, 5ê¸°ì¤€ fstats 29.9136

# COMMAND ----------

# game -> buyers 4~15 ê·€ë¬´ê°€ì„¤ ê¸°ê°
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(dataM_median[['all_unique_buyers', 'game_average_usd']]['2018':], maxlag=15)

# COMMAND ----------

# buyers -> game  6~14 ê·€ë¬´ê°€ì„¤ ê¸°ê°
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(dataM_median[['game_average_usd', 'all_unique_buyers']]['2018':], maxlag=15)

# COMMAND ----------

# MAGIC %md
# MAGIC ##### all_buyers & game_avgusd
# MAGIC - buyers -> game  6~14 ê·€ë¬´ê°€ì„¤ ê¸°ê°, 6ê¸°ì¤€ fstats 4.3648
# MAGIC - game -> buyers 4~15 ê·€ë¬´ê°€ì„¤ ê¸°ê°, 6ê¸°ì¤€ fstats 39.6156

# COMMAND ----------

# MAGIC %md
# MAGIC ##### ì œ3ì˜ ì™¸ë¶€ ë³€ìˆ˜ ì¶”ê°€
# MAGIC - ê°€ê²© í˜•ì„± ìš”ì¸ìœ¼ë¡œ ì™¸ë¶€ ì´ìŠˆ(ì–¸ë¡ , í™ë³´, ì»¤ë®¤ë‹ˆí‹°) ìš”ì¸ìœ¼ë¡œ ì¶”ì •ë¨
# MAGIC - ì»¤ë®¤ë‹ˆí‹° ë°ì´í„°(ex: nft tweet)ë¥¼ êµ¬í•˜ì§€ ëª»í•´ í¬í„¸ ê²€ìƒ‰ ë°ì´í„°(rate, per week)ë¥¼ ëŒ€ì•ˆìœ¼ë¡œ ë¶„ì„í•´ë³´ì

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ë¯¸ë‹ˆ EDA
# MAGIC - ì£¼ë‹¨ìœ„ ìˆ˜ì¹˜í˜• "ë¹„ìœ¨" ë°ì´í„°
# MAGIC - 1%ë¯¸ë§Œì€ 1ìœ¼ë¡œ ì‚¬ì „ì— ë³€ê²½

# COMMAND ----------

gtkwd_data = pd.read_csv('/dbfs/FileStore/nft/google_trend/nft_googletrend_w_170423_220423.csv', index_col = "Date", parse_dates=True, thousands=',')

# COMMAND ----------

gtkwd_data.info()

# COMMAND ----------

gtkwd_data.rename(columns={'nft':'nft_gt'}, inplace=True)
gtkwd_data.describe()

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ë¯¸ë‹ˆ ì‹œê°í™”
# MAGIC - ë¶„í¬ : 1ì´ 77%
# MAGIC - ì¶”ì„¸ : 2021ë…„ 1ì›”ë¶€í„° ê¸‰ë“±í•´ì„œ 6ì›”ê¹Œë¼ ê¸‰ë½í–ˆë‹¤ê°€ 22ë…„1ì›”ê¹Œì§€ ë‹¤ì‹œ ê¸‰ë“± ì´í›„ í•˜ë½ì„¸
# MAGIC - ë²”ìœ„ : 21ë…„ë„ ì´í›„ iqrë²”ìœ„ëŠ” 10~40, ì¤‘ìœ„ê°’ì€ ì•½25, ìµœëŒ€ ì•½ 85, 

# COMMAND ----------

gtkwd_dataM_median = gtkwd_data.resample('M').median()
gtkwd_dataM_median.tail()

# COMMAND ----------

plt.figure(figsize=(30,10))

plt.subplot(2, 2, 1)   
plt.title('<weekly_raw>', fontsize=22)
plt.hist(gtkwd_data)

plt.subplot(2, 2, 2)
plt.title('<monthly_median>', fontsize=22)
plt.hist(gtkwd_dataM_median)

plt.show()

# COMMAND ----------

plt.figure(figsize=(30,10))

plt.subplot(2, 2, 1)   
plt.title('<weekly_raw>', fontsize=22)
plt.plot(gtkwd_data)

plt.subplot(2, 2, 2)
plt.title('<monthly_median>', fontsize=22)
plt.plot(gtkwd_dataM_median)

plt.show()

# COMMAND ----------

plt.figure(figsize=(30,10))

plt.subplot(2, 2, 1)   
plt.title('<weekly_raw>', fontsize=22)
plt.boxplot(gtkwd_data['2021':])

plt.subplot(2, 2, 2)
plt.title('<monthly_median>', fontsize=22)
plt.boxplot(gtkwd_dataM_median['2021':])

plt.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ë°ì´í„° í†µí•©

# COMMAND ----------

marketdataM = data['2018':'2022-01'].resample('M').median()
marketdataM.tail()

# COMMAND ----------

# ì›”ê°„ í†µí•©
total = pd.merge(marketdataM, gtkwd_dataM, left_index=True, right_index=True, how='left')
total.tail()

# COMMAND ----------

# ì£¼ê°„ í†µí•©
marketdataW = data['2018':'2022-01'].resample('W').median()
totalW = pd.merge(marketdataW, gtkwd_data, left_index=True, right_index=True, how='left')
totalW.tail()

# COMMAND ----------

# ì£¼ê°„ í†µí•©
total = pd.merge(marketdata, gtkwd_data, left_index=True, right_index=True, how='left')
total.tail()

# COMMAND ----------

# ì •ê·œí™”
from sklearn.preprocessing import MinMaxScaler
minmax_scaler = MinMaxScaler()
total_scaled = total.copy()
total_scaled.iloc[:,:] = minmax_scaler.fit_transform(total_scaled)
total_scaled.describe()

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ë¯¸ë‹ˆ ìƒê´€ë¶„ì„
# MAGIC - í™•ì¸ê²°ê³¼ ìŠ¤ì¼€ì¼ë§ ì •ê·œí™”ë‘ ì°¨ì´ ì—†ìŒ, rawë°ì´í„°ë¡œ ë³´ë©´ë¨, ì›”ê°„ê³¼ ì£¼ê°„ ì°¨ì´ ì—†ìŒ

# COMMAND ----------

# [í•¨ìˆ˜] ì¹´í…Œê³ ë¦¬ë³„ íˆíŠ¸ë§µ ìƒì„±ê¸°
import plotly.figure_factory as ff

# ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸°
def category_classifier(data, category):
    col_list = []
    for i in range(len(data.columns)):
        if data.columns[i].split('_')[0] == category:
            col_list.append(data.columns[i])
        else :
            pass
    return col_list

def heatmapC(data, category):
    # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ê¸° í˜¸ì¶œ
    col_list = category_classifier(data, category)
    col_list.append('nft_gt')
    
    # ì‚¼ê°í–‰ë ¬ ë°ì´í„° ë° mask ìƒì„±
    corr = round(data[col_list].corr(), 2)
    mask = np.triu(np.ones_like(corr, dtype=bool))
    # ìƒë¶€ ì‚¼ê°í–‰ë ¬ ìƒì„±(np.tilu()ì€ í•˜ë¶€), np.ones_like(bool)ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ê°’ì´ ìˆëŠ” í•˜ë¶€ì‚¼ê°í–‰ë ¬ì€ 1(true)ë¥¼ ë°˜í™˜í•œë‹¤.
    # í•˜ë¶€ë¥¼ ë§Œë“¤ë©´ ìš°ì¸¡ê¸°ì¤€ìœ¼ë¡œ ìƒì„±ë˜ê¸° ë•Œë¬¸ì— ì™¼ìª½ê¸°ì¤€ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ìƒë¶€ë¥¼ ë°˜ì „í•œë‹¤.
    df_mask = corr.mask(mask)

    
    fig = ff.create_annotated_heatmap(z=df_mask.to_numpy(), 
        x=df_mask.columns.tolist(),
        y=df_mask.columns.tolist(),
        colorscale='Blues',
        hoverinfo="none", #Shows hoverinfo for null values
        showscale=True,
        xgap=3, ygap=3, # margin
        zmin = 0, zmax=1     
        )
    
    fig.update_xaxes(side="bottom") # xì¶•íƒ€ì´í‹€ì„ í•˜ë‹¨ìœ¼ë¡œ ì´ë™

    fig.update_layout(
        title_text='<b>Correlation Matrix (ALL ì¹´í…Œê³ ë¦¬ í”¼ì²˜ê°„ ìƒê´€ê´€ê³„)<b>', 
        title_x=0.5, 
#         width=1000, height=1000,
        xaxis_showgrid=False,
        yaxis_showgrid=False,
        xaxis_zeroline=False,
        yaxis_zeroline=False,
        yaxis_autorange='reversed', # í•˜ë‹¨ ì‚¼ê°í˜•ìœ¼ë¡œ ë³€ê²½
        template='plotly_white'
    )

    # NaN ê°’ì€ ì¶œë ¥ì•ˆë˜ë„ë¡ ìˆ¨ê¸°ê¸°
    for i in range(len(fig.layout.annotations)):
        if fig.layout.annotations[i].text == 'nan':
            fig.layout.annotations[i].text = ""

    fig.show()
    

# COMMAND ----------

# [í•¨ìˆ˜] í”¼ì²˜ë³„ íˆíŠ¸ë§µ ìƒì„±ê¸°
import plotly.figure_factory as ff

def heatmapF(data, feature):
    # í”¼ì²˜ ë¶„ë¥˜ê¸° í˜¸ì¶œ
    col_list = feature_classifier(data, feature)
    col_list.append('nft_gt')
     # all ì¹´í…Œê³ ë¦¬ ì œì™¸
#     new_col_list = []
#     for col in col_list:
#         if col.split('_')[0] != 'all':
#             new_col_list.append(col)
#         else: pass
    
    corr = round(data[col_list].corr(), 2)
        
    # ì‚¼ê°í–‰ë ¬ ë°ì´í„° ë° mask ìƒì„±
    mask = np.triu(np.ones_like(corr, dtype=bool))
    # ìƒë¶€ ì‚¼ê°í–‰ë ¬ ìƒì„±(np.tilu()ì€ í•˜ë¶€), np.ones_like(bool)ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ê°’ì´ ìˆëŠ” í•˜ë¶€ì‚¼ê°í–‰ë ¬ì€ 1(true)ë¥¼ ë°˜í™˜í•œë‹¤.
    # í•˜ë¶€ë¥¼ ë§Œë“¤ë©´ ìš°ì¸¡ê¸°ì¤€ìœ¼ë¡œ ìƒì„±ë˜ê¸° ë•Œë¬¸ì— ì™¼ìª½ê¸°ì¤€ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ìƒë¶€ë¥¼ ë°˜ì „í•œë‹¤.
   
    df_mask = corr.mask(mask)

    
    fig = ff.create_annotated_heatmap(z=df_mask.to_numpy(), 
        x=df_mask.columns.tolist(),
        y=df_mask.columns.tolist(),
        colorscale='Blues',
        hoverinfo="none", #Shows hoverinfo for null values
        showscale=True,
        xgap=3, ygap=3, # margin
        zmin = 0, zmax=1     
        )
    
    fig.update_xaxes(side="bottom") # xì¶•íƒ€ì´í‹€ì„ í•˜ë‹¨ìœ¼ë¡œ ì´ë™

    fig.update_layout(
        title_text='<b>Correlation Matrix ("average USD"í”¼ì²˜, ì¹´í…Œê³ ë¦¬ê°„ ìƒê´€ê´€ê³„)<b>', 
        title_x=0.5, 
#         width=1000, height=1000,
        xaxis_showgrid=False,
        yaxis_showgrid=False,
        xaxis_zeroline=False,
        yaxis_zeroline=False,
        yaxis_autorange='reversed', # í•˜ë‹¨ ì‚¼ê°í˜•ìœ¼ë¡œ ë³€ê²½
        template='plotly_white'
    )

    # NaN ê°’ì€ ì¶œë ¥ì•ˆë˜ë„ë¡ ìˆ¨ê¸°ê¸°
    for i in range(len(fig.layout.annotations)):
        if fig.layout.annotations[i].text == 'nan':
            fig.layout.annotations[i].text = ""

    fig.show()
    

# COMMAND ----------

# nft_gtì™€ ëŒ€ì²´ë¡œ ìƒê´€ê´€ê³„ê°€ ë†’ìŒ, utilityì œì™¸(collectibleì´ ê°€ì¥ ë†’ìŒ)
heatmapC(total, 'all')

# COMMAND ----------

# ëŒ€ì²´ë¡œ ìƒê´€ê´€ê³„ê°€ ë†’ë‹¤. ìƒëŒ€ì ìœ¼ë¡œ avgusdê°€ ë‚®ì§€ë§Œ ê·¸ë˜ë„ ë†’ì€í¸ì´ë‹ˆ ì¸ê³¼ê²€ì • í• ë§Œ í•œë“¯(ì•„ë¬´ë¦¬ ìƒê°í•´ë„ nftê°€ê²©ì€...ì»¤ë®¤ë‹ˆí‹°ì˜í–¥ì´ í´ ê²ƒ ê°™ì€ë°.. nft tweet ë°ì´í„°ê°€ ì—†ì–´ì„œ ì•„ì‰½ë‹¤.)
heatmapC(total['2021':], 'all')

# COMMAND ----------

# nft_gtì™€ ëŒ€ì²´ë¡œ ìƒê´€ê´€ê³„ê°€ ë†’ìŒ, utilityì œì™¸(collectibleì´ ê°€ì¥ ë†’ìŒ) í•˜ì§€ë§Œ, 2018~2020ê¹Œì§€ ëª¨ë‘ 1ì´ë¼ì„œ íŒë‹¨í•˜ê¸° ì–´ë ¤ì›€
heatmapF(total, 'average_usd')

# COMMAND ----------

# ë³¸ê²©ì ìœ¼ë¡œ ê²€ìƒ‰ëŸ‰ì´ ë§ì•„ì§„ 21ë…„ë„ë¶€í„° ì°¨ì´ê°€ í™•ì—°í•˜ë‹¤.
# allê¸°ì¤€ ê²€ìƒ‰ëŸ‰ê³¼ ìƒê´€ê´€ê³„ê°€ ë†’ì€í¸, metaverse, collectible, artê°€ ê°€ì¥ ë†’ê³ , defiëŠ” ë‚®ì€ ìˆ˜ì¤€. collectible, gameê³¼ ì¸ê³¼ê²€ì •í•´ë³´ì
heatmapF(total['2021':], 'average_usd')

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ì‹œì°¨ìƒê´€ë¶„ì„

# COMMAND ----------

nftgt_list = ['nft_gt', 'collectible_average_usd', 'game_average_usd', 'all_average_usd', 'all_unique_buyers']

# COMMAND ----------

# ì›” ì¤‘ì•™ê°’, 2021ë…„ë„ ì´í›„
print(f"<<<Xê¸°ì¤€ Yì˜ ë³€ë™í­ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜ í…Œì´ë¸”>>>")
result_df = TLCC_comparison_table(total['2021':], 'nft_gt', nftgt_list, -6, 6)
result_df

# COMMAND ----------

## ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼(ìƒ‰ êµ¬ê°„ ì„¤ì • í•´ì•¼í•¨, ë³¼ ë•Œ ìœ ì˜)
pd.set_option('display.precision', 2) # ì†Œìˆ˜ì  ê¸€ë¡œë²Œ ì„¤ì •
result_df.style.background_gradient(cmap='Blues').set_caption(f"<b><<<'X(0)ê¸°ì¤€ Yì˜ ë³€ë™í­ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜'>>><b>")

# COMMAND ----------

# ì£¼ê°„ ê¸°ì¤€, 2021ë…„ë„ ì´í›„
print(f"<<<Xê¸°ì¤€ Yì˜ ë³€ë™í­ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜ í…Œì´ë¸”>>>")
result_df = TLCC_comparison_table(totalW['2021':], 'nft_gt', nftgt_list, -12, 12)
result_df

# COMMAND ----------

## ë°ì´í„°í”„ë ˆì„ ìŠ¤íƒ€ì¼(ìƒ‰ êµ¬ê°„ ì„¤ì • í•´ì•¼í•¨, ë³¼ ë•Œ ìœ ì˜)
pd.set_option('display.precision', 2) # ì†Œìˆ˜ì  ê¸€ë¡œë²Œ ì„¤ì •
result_df.style.background_gradient(cmap='Blues').set_caption(f"<b><<<'X(0)ê¸°ì¤€ Yì˜ ë³€ë™í­ ë° ì‹œì°¨ìƒê´€ê³„ìˆ˜'>>><b>")

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ã„´ nft&cau&gau ì‹œì°¨ìƒê´€ë¶„ì„(2021ë…„ ì´í›„ ì›”ì¤‘ì•™ê°’ & ì£¼ê°„)
# MAGIC - nftgt & nftgt : +- 3ê°œì›” ì •ë„ ìƒê´€ì„±ì´ ìˆìŒ
# MAGIC - nftgt & cau : -2ê°œì›”ë¶€í„° ìƒê´€ì„±ì´ ë†’ê³ , +ëŠ” ë§¤ìš° ë†’ìŒ, nftgt -> cauê´€ê³„ë¡œ ì¶”ì •
# MAGIC - nftgt & gau : 1ë¶€í„° ìƒê´€ì„±ì´ ë†’ìŒìœ¼ë‚˜ cauì— ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ nftgt -> cauê´€ê³„ë¡œ ì¶”ì •
# MAGIC - nftgt & au : 0ë¶€í„° ë†’ìŒ, nftgt -> auê´€ê³„ë¡œ ì¶”ì •
# MAGIC - nftgt & ub : -2 ~ 0ë†’ì•˜ì€ë°, 1~2ì— ì ì‹œ í•˜ë½í–ˆë‹¤ê°€ ê¸‰ë“±, ub->nftgt ê´€ê³„ì¸ê°€? ë­ì§€??

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ê³µì ë¶„ ê²€ì •
# MAGIC - ì•µê¸€&ê·¸ë ˆì¸ì €, ì£¼ê°„ë°ì´í„°
# MAGIC - nftgt ì™€ ì‹œì°¨ìƒê´€ì„±ì´ ë†’ì€ cauì™€ aubë§Œ ëŒ€í‘œë¡œ ë³´ì

# COMMAND ----------

# ê³µì ë¶„ ê´€ê³„ ì‹œê°í™”
X = totalW['nft_gt']['2021':]
Y = totalW['collectible_average_usd']['2021':]

# ë””í´íŠ¸ : rawë°ì´í„°(ë¡œê·¸ë³€í™˜/ìŠ¤ì¼€ì¼ë§ë“± ì •ê·œí™”í•˜ë©´ ì•ˆë¨, íŠ¹ì§• ì‚¬ë¼ì§), augmented engle&granger(default), maxlag(none), trend='c'
import statsmodels.tsa.stattools as ts
score, pvalue, _ = ts.coint(X,Y)
print('Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('ADF score: ' + str( np.round(score, 4) ))
print('Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ìƒìˆ˜&ê¸°ìš¸ê¸°')
score, pvalue, _ = ts.coint(X,Y, trend='ct')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ìƒìˆ˜&ê¸°ìš¸ê¸°(2ì°¨)')
score, pvalue, _ = ts.coint(X,Y, trend='ctt')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ì—†ìŒ')
score, pvalue, _ = ts.coint(X,Y, trend='nc')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))

(Y/X).plot(figsize=(30,10))
plt.axhline((Y/X).mean(), color='red', linestyle='--')
plt.xlabel('Time')
plt.title('collectible_avgusd / nft_gt Ratio')
plt.legend(['collectible_avgusd / nft_gt Ratio', 'Mean'])
plt.show()

# COMMAND ----------

# ê³µì ë¶„ ê´€ê³„ ì‹œê°í™”
X = totalW['nft_gt']['2021':]
Y = totalW['all_unique_buyers']['2021':]

# ë””í´íŠ¸ : rawë°ì´í„°(ë¡œê·¸ë³€í™˜/ìŠ¤ì¼€ì¼ë§ë“± ì •ê·œí™”í•˜ë©´ ì•ˆë¨, íŠ¹ì§• ì‚¬ë¼ì§), augmented engle&granger(default), maxlag(none), trend='c'
import statsmodels.tsa.stattools as ts
score, pvalue, _ = ts.coint(X,Y)
print('Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('ADF score: ' + str( np.round(score, 4) ))
print('Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ìƒìˆ˜&ê¸°ìš¸ê¸°')
score, pvalue, _ = ts.coint(X,Y, trend='ct')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ìƒìˆ˜&ê¸°ìš¸ê¸°(2ì°¨)')
score, pvalue, _ = ts.coint(X,Y, trend='ctt')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))
print('='*50)

print('ì¶”ì„¸ ì—†ìŒ')
score, pvalue, _ = ts.coint(X,Y, trend='nc')
print('Rawdata Correlation: ' + str( np.round(X.corr(Y), 4) ))
print('Rawdata ADF score: ' + str( np.round(score, 4) ))
print('Rawdata Cointegration test p-value: ' + str( np.round(pvalue, 4) ))

(Y/X).plot(figsize=(30,10))
plt.axhline((Y/X).mean(), color='red', linestyle='--')
plt.xlabel('Time')
plt.title('all_buyers / nft_gt Ratio')
plt.legend(['all_buyers / nft_gt', 'Mean'])
plt.show()

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ã„´ ì•µê¸€&ê·¸ë ˆì¸ì € ê²€ì • ê²°ê³¼(ì£¼ê°„)
# MAGIC - nftgt & cau : cttê¸°ì¤€ pval  0.3798ë¡œ 0.05ë¥¼ ì´ˆê³¼í•˜ì—¬ ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•˜ì—¬ **ê³µì ë¶„ê´€ê³„ ì—†ìŒ**
# MAGIC - nftgt & ub : cttê¸°ì¤€ pval 0.4232 ë¡œ 0.05ë¥¼ ì´ˆê³¼í•˜ì—¬ ê·€ë¬´ê°€ì„¤ì„ ì±„íƒí•˜ì—¬ **ê³µì ë¶„ê´€ê³„ ì—†ìŒ** 

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ê·¸ë ˆì¸ì € ì¸ê³¼ê²€ì •
# MAGIC - nftgt ì™€ ì‹œì°¨ìƒê´€ì„±ì´ ë†’ì€ cauì™€ aubë§Œ ëŒ€í‘œë¡œ ë³´ì
# MAGIC - ì›”ì¤‘ì•™ê°’ìœ¼ë¡œ ë³´ë©´ ì¸ê³¼ê²€ì • ì‹¤íŒ¨í•˜ì—¬, ì£¼ê°„ìœ¼ë¡œ ë‹¤ì‹œ ë´„
# MAGIC - 

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ã„´ nft_gt & cau(ì£¼ê°„)

# COMMAND ----------

# nft_gt -> cau, ì£¼ê°„
# fê²€ì • pvalì´ 0.05ì´ˆê³¼í•˜ì—¬ ê·€ë¬´ê°€ì„¤ ì±„íƒ, ì¸ê³¼ê´€ê³„ ì—†ìŒ, ê·¸ë‚˜ë§ˆ 2ê°€ 0.06ìœ¼ë¡œ ê°€ê¹Œìš´í¸
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(totalW[['collectible_average_usd', 'nft_gt']]['2021':], maxlag=12)

# COMMAND ----------

# cau -> nft_gt, ì£¼ê°„
# 1~2ê°€ fê²€ì • pvalì´ 0.05ë¯¸ë§Œìœ¼ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°, ê·¸ë ˆì¸ì € ì¸ê³¼ê²€ì • í†µê³¼
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(totalW[['nft_gt', 'collectible_average_usd']]['2021':], maxlag=12)

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ã„´ nft_gt & gau(ì£¼ê°„)

# COMMAND ----------

# nft_gt -> gau, ì£¼ê°„
# 1ì£¼ fê²€ì • pvalì´ 0.05ë¯¸ë§Œìœ¼ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°, ì¸ê³¼ê²€ì • í†µê³¼
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(totalW[['game_average_usd', 'nft_gt']]['2021':], maxlag=12)

# COMMAND ----------

# gau -> nft_gt, ì£¼ê°„
# 3ì£¼ fê²€ì • pvalì´ 0.05ë¯¸ë§Œìœ¼ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°, ì¸ê³¼ê²€ì • í†µê³¼
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(totalW[['nft_gt', 'game_average_usd']]['2021':], maxlag=12)

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ã„´ nft_gt & aau(ì£¼ê°„)

# COMMAND ----------

# nft_gt -> aau, ì£¼ê°„
# fê²€ì • pvalì´ 0.05ì´ˆê³¼í•˜ì—¬ ê·€ë¬´ê°€ì„¤ ì±„íƒ, ì¸ê³¼ê´€ê³„ ì—†ìŒ, ê·¸ë‚˜ë§ˆ 1ê°€ 0.09ìœ¼ë¡œ ê°€ê¹Œìš´í¸
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(totalW[['all_average_usd', 'nft_gt']]['2021':], maxlag=12)

# COMMAND ----------

# aau -> nft_gt, ì£¼ê°„
# 1,2,3,12 fê²€ì • pvalì´ 0.05ë¯¸ë§Œìœ¼ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°, ì¸ê³¼ê²€ì • í†µê³¼ ì—†ìŒ
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(totalW[['nft_gt', 'all_average_usd']]['2021':], maxlag=12)

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ã„´ nft_gt & ub(ì£¼ê°„)

# COMMAND ----------

# nft_gt -> aub
# 1~2,7 ì£¼ ê°€ fê²€ì • pvalì´ 0.05ë¯¸ë§Œìœ¼ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°í•˜ì—¬ ì¸ê³¼ê²€ì • í†µê³¼
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(totalW[['all_unique_buyers', 'nft_gt']]['2021':], maxlag=12)

# COMMAND ----------

# aub -> nft_gt
# fê²€ì • pvalì´ 0.05ì´ˆê³¼ë¡œ ê·€ë¬´ê°€ì„¤ ì±„íƒí•˜ì—¬ ì¸ê³¼ê²€ì • ë¶ˆí†µ
from statsmodels.tsa.stattools import grangercausalitytests
grangercausalitytests(totalW[['nft_gt', 'all_unique_buyers']]['2021':], maxlag=12)

# COMMAND ----------

# MAGIC %md
# MAGIC ###### ã„´ì™¸ë¶€ë³€ìˆ˜ ì¸ê³¼ê²€ì • ê²°ê³¼
# MAGIC - ì›”ê°„ìœ¼ë¡œ ìƒê´€ì„± ë° ì‹œì°¨ìƒê´€ì„±ì€ ë†’ì•˜ìŒì—ë„ ì¸ê³¼ê²€ì • ì‹œ ëª¨ë‘ ì¸ê³¼ì„± ì—†ì—ˆìŒ
# MAGIC - í•´ì„ì´ ì–´ë ¤ì› ëŠ”ë°, ë°ì´í„° ì •ë³´ ì†ì‹¤ ë¬¸ì œ(ë¹„ìœ¨ì„ ì›”ê°„ì¤‘ì•™ê°’ìœ¼ë¡œ ê°€ê³µ) ë˜ëŠ” ì œ 3ì˜ ìš”ì¸ìœ¼ë¡œ ì¶”ì •(ì»¤ë®¤ë‹ˆí‹° ë°ì´í„°) 
# MAGIC - ìµœëŒ€í•œ nft_gtë°ì´í„° ì •ë³´ë¥¼ ì‚´ë¦¬ê¸° ìœ„í•´ ì£¼ê°„ìœ¼ë¡œ ë‹¤ì‹œ ê²€ì • ê²°ê³¼
# MAGIC   - nft_gt -> cau : ì¸ê³¼ì˜í–¥ ì—†ìŒ, ê·¸ë‚˜ë§ˆ 2ê°€ 0.06ìœ¼ë¡œ ê°€ê¹Œìš´í¸
# MAGIC   - cau -> nft_gt : 1, 2 ì¸ê³¼ì˜í–¥ ìˆìŒ
# MAGIC   - nft_gt -> gau : 1 ì¸ê³¼ì˜í–¥ ìˆìŒ
# MAGIC   - gau -> nft_gt : 3 ì¸ê³¼ì˜í–¥ ìˆìŒ
# MAGIC   - nft_gt -> aau : ì¸ê³¼ì˜í–¥ ì—†ìŒ, ê·¸ë‚˜ë§ˆ 1ê°€ 0.09ìœ¼ë¡œ ê°€ê¹Œìš´í¸
# MAGIC   - aau -> nft_gt : 1,2,3,12 ì¸ê³¼ì˜í–¥ ìˆìŒ
# MAGIC   - nft_gt -> aub : 1,2,7 ì¸ê³¼ì˜í–¥ ìˆìŒ
# MAGIC   - aub -> nft_gt : ì¸ê³¼ì˜í–¥ ì—†ìŒ

# COMMAND ----------

# MAGIC %md
# MAGIC ##### <ê²€ì •ê²°ê³¼ì¢…í•©>
# MAGIC - [ë„í‘œ ë¬¸ì„œ](https://docs.google.com/presentation/d/1_XOsoLV95qqUwJI8kxFXS_7NUIQbp872UHT_cQ162Us/edit#slide=id.g122453ac673_0_0)
# MAGIC - 1) game â†’ buyers/collectible
# MAGIC - 2) buyers â†’ collectible
# MAGIC - 3) collectible â†’all
# MAGIC - 4) all â†’ buyers 
# MAGIC - ê²°ê³¼ì ìœ¼ë¡œ ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ë¶„ì„ì€.. ì–´ë–¤ ë³€ìˆ˜ë¡œ ë¬´ì—‡ì„ ì˜ˆì¸¡í•´ì•¼í• ê¹Œ?

# COMMAND ----------

# MAGIC %md
# MAGIC ## ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ ë¶„ì„
# MAGIC - ê³µì ë¶„ ë¯¸ì¡´ì¬ì‹œ VAR -> ìš”í•œìŠ¨ê²€ì • -> ê³µì ë¶„ ì¡´ì¬ì‹œ VECM

# COMMAND ----------

# MAGIC %md
# MAGIC ### ê³µì ë¶„ ë¯¸ì¡´ì¬ì‹œ VAR(ë²¡í„°ìê¸°íšŒê·€ëª¨í˜•)

# COMMAND ----------

# MAGIC %md
# MAGIC ### (ë‹¤ë³€ëŸ‰)Johansen Test
# MAGIC - VARëª¨í˜•ì— ëŒ€í•œ ê°€ì„¤ê²€ì •ì„ í†µí•´ ì ë¶„ê³„ì—´ê°„ ì•ˆì •ì ì¸ ì¥ê¸°ê· í˜•ê´€ê³„ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ì ê²€í•˜ëŠ” ë°©ë²•
# MAGIC - 3ê°œ ì´ìƒì˜ ë¶ˆì•ˆì • ì‹œê³„ì—´ ì‚¬ì´ì˜ ê³µì ë¶„ ê²€ì •ì— í•œê³„ë¥¼ ê°–ëŠ” ì•µê¸€&ê·¸ë Œì € ê²€ì • ë°©ë²•ì„ ê°œì„ í•˜ì—¬ ë‹¤ë³€ëŸ‰ì—ë„ ê³µì ë¶„ ê²€ì •ì„ í•  ìˆ˜ ìˆìŒ
# MAGIC - statsmodels.tsa.vector_ar.vecm. coint_johansen 
# MAGIC   - VECMì˜ ê³µì ë¶„ ìˆœìœ„ì— ëŒ€í•œ ìš”í•œì„¼ ê³µì ë¶„ ê²€ì •
# MAGIC   - [signature](https://www.statsmodels.org/dev/generated/statsmodels.tsa.vector_ar.vecm.coint_johansen.html)

# COMMAND ----------

from statsmodels.tsa.vector_ar.vecm import coint_johansen

# COMMAND ----------



# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC ### ê³µì ë¶„ ì¡´ì¬ì‹œ VECM(ë²¡í„°ì˜¤ì°¨ìˆ˜ì •ëª¨í˜•)
# MAGIC - ë¶ˆì•ˆì •ì‹œê³„ì—´Xì™€ Yë¥¼ 1ì°¨ ì°¨ë¶„í•œ ë³€ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨ ì „í†µì  ë°©ë²•ì˜ ì‚¬ìš©ìœ¼ë¡œ ì¸í•´ ì•¼ê¸°ë˜ëŠ” ë¬¸ì œì ë“¤ì„ ì–´ëŠì •ë„ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë‚˜, ë‘ ë³€ìˆ˜ ê°™ì˜ ì¥ê¸°ì  ê´€ê³„ì— ëŒ€í•œ ì†Œì¤‘í•œ ì •ë³´ë¥¼ ìƒì‹¤í•˜ê²Œ ëœë‹¤.
# MAGIC - ì´ ê²½ìš° ë§Œì¼ ë‘ ë³€ìˆ˜ ê°„ì— ê³µì ë¶„ì´ ì¡´ì¬í•œë‹¤ë©´ ì˜¤ì°¨ìˆ˜ì •ëª¨í˜•(error correction model)ì„ í†µí•´ ë³€ìˆ˜ë“¤ì˜ ë‹¨ê¸°ì  ë³€ë™ë¿ë§Œ ì•„ë‹ˆë¼ ì¥ê¸°ê· í˜•ê´€ê³„ì— ëŒ€í•œ íŠ¹ì„±ì„ ì•Œ ìˆ˜ ìˆê²Œ ëœë‹¤.
# MAGIC - VECMì€ ì˜¤ì°¨ìˆ˜ì •ëª¨í˜•(ECM)ì— ë²¡í„°ìê¸°íšŒê·€ëª¨í˜•(VAR)ê³¼ ê°™ì€ ë‹¤ì¸ì ëª¨í˜• ê°œë…ì„ ì¶”ê°€ í•œ ê²ƒ
# MAGIC - [VECM ì˜ˆì œ](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=gush14&logNo=120145414589)
# MAGIC - [íŒŒì´ì¬ ì˜ˆì œ](http://incredible.ai/trading/2021/07/01/Pair-Trading/)

# COMMAND ----------



# COMMAND ----------

# MAGIC %md
# MAGIC ## ì¶©ê²©ë°˜ì‘ë¶„ì„

# COMMAND ----------



# COMMAND ----------



# COMMAND ----------

# ## ê³µì ë¶„ ê²€ì¦ê¸°(ì¹´í…Œê³ ë¦¬)
# def coint_table(data, col_list, nlag):

#     xcol_list = []
#     ycol_list = []
#     pval_list = []
# #     havetomoreX = []
# #     havetomoreY = []
    
#     for i in range(len(col_list)):
#         for j in range(1, len(col_list)):
#             if col_list[i] == col_list[j]:
#                 pass
#             else:
#                 xcol_list.append(col_list[i])
#                 ycol_list.append(col_list[j])
#                 _, pval = coint_lag(data[col_list[i]], data[col_list[j]], nlag)
#                 pval_list.append(pval)
                
#                 print(col_list[i], '|', col_list[j] )
#                 print(pval)

# COMMAND ----------

# coint_table(data, avgusd_col_list, 14)

# COMMAND ----------

# # avgusdí”¼ì²˜ì˜ ì¹´í…Œê³ ë¦¬ê°„ ê³µì ë¶„ ê²€ì¦

# avgusd_col_list
# for 






# COMMAND ----------

# allì¹´í…Œê³ ë¦¬ í”¼ì²˜ê°„ ê³µì ë¶„ ê²€ì¦

# COMMAND ----------

# ## TLCC table ìƒì„±ê¸°
# def TLCC_table(data, col_list, nlag):

#     xcol_list = []
#     ycol_list = []
#     TLCC_list = []
#     havetomoreX = []
#     havetomoreY = []

#     for i in range(len(col_list)):
#         for j in range(1, len(col_list)):
#             if col_list[i] == col_list[j]:
#                 pass
#             else:
#                 xcol_list.append(col_list[i])
#                 ycol_list.append(col_list[j])
#                 tlccdata = TLCC(data[col_list[i]], data[col_list[j]], nlag)
#                 TLCC_list.append(tlccdata)
# #                 print(col_list[i], col_list[j])
# #                 print(tlccdata)
# #                 print(np.argmax(tlccdata))
# #                 print(np.argmax(TLCC_list[i]))
#                 max_TLCC_idx = np.argmax(tlccdata)
#                 max_TLCC = np.round(max(tlccdata),4)
#                 if max_TLCC >= 0.7:
#                     result = 'ë†’ìŒ'
#                 elif max_TLCC > 0.3 and max_TLCC < 0.7:
#                     result = 'ë³´í†µ'
#                 else :
#                     result = 'ë‚®ìŒ'
#                 print(col_list[i], '|', col_list[j], '|', max_TLCC_idx, '|', max_TLCC, '|', result)
            
                
#                 if max_TLCC_idx == nlag-1:
#                     havetomoreX.append(col_list[i])
#                     havetomoreY.append(col_list[j])

#     return havetomoreX, havetomoreY
